{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as udata\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuarcy(dataloader): \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #print(inputs.size())\n",
    "            #print(\"labels: \", labels, labels.size(0))\n",
    "            outputs = cnn(inputs)\n",
    "            #print(outputs.data)\n",
    "            #might change\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            probability = softmax(outputs.data)\n",
    "            _, predicted = torch.max(probability, 1)\n",
    "            #print(\"pred: \", predicted)\n",
    "            total += labels.size(0)\n",
    "            #print(\"total: \", total)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(\"correct: \", correct)\n",
    "            if total % 100 == 98:\n",
    "                print(total)\n",
    "    #print(total)    \n",
    "    #print('Accuracy of the network on the train data: %d %%' % (\n",
    "        #100 * correct / total))\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cnn, trainloder, testloader, num, epsilon=0.003, filename = None):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn.parameters(), lr= 0.001)\n",
    "    pre_epoch = float(\"inf\")\n",
    "    for epoch in range(num):\n",
    "        running_loss = 0.0\n",
    "        total = 0 \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            #print(inputs.size())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn(inputs)\n",
    "            #print(inputs.size(), outputs.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            total += 1\n",
    "        test_accuarcy = accuarcy(testloader)\n",
    "        train_accuarcy = accuarcy(trainloader)\n",
    "        epoch_loss = running_loss / total\n",
    "        print('epoch [%d] loss: %.3f' % (epoch + 1, epoch_loss), train_accuarcy, test_accuarcy)\n",
    "        if filename != None:\n",
    "            f = open(filename, 'a')\n",
    "            f.write('epoch {} loss: {}, train_accuracy: {}, test_accuracy: {} \\n'.format(epoch+1, epoch_loss, train_accuarcy, test_accuarcy))\n",
    "        pre_epoch = epoch_loss\n",
    "        \n",
    "    print('Finished Training')\n",
    "    if filename != None:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(use_singan, dir_name):\n",
    "    #load train set\n",
    "    trainpath = \"cifar10-2classes-trainset\"\n",
    "    transformer = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    if use_singan:\n",
    "        #trainpath = \"singan-train\"\n",
    "        trainset1 = datasets.ImageFolder(root=trainpath, transform=transformer)\n",
    "        trainset2 = datasets.ImageFolder(root=\"singan-train\", transform=transformer)\n",
    "        trainset = udata.ConcatDataset([trainset1, trainset2])\n",
    "    else:\n",
    "        trainset = datasets.ImageFolder(root=trainpath, transform=transformer)\n",
    "    \n",
    "    #load test set\n",
    "    testpath = \"cifar10-2classes-testset\"\n",
    "    testset = datasets.ImageFolder(root=testpath, transform=transformer)\n",
    "    \n",
    "    #set up device and train/test loader\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainloader = udata.DataLoader(trainset, batch_size = 20, shuffle=True)\n",
    "    testloader = udata.DataLoader(testset, batch_size = 20, shuffle=True)\n",
    "    \n",
    "    #Build CNN\n",
    "    cnn = CNN()\n",
    "    cnn.to(device)\n",
    "    \n",
    "    return cnn, trainloader, testloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586387564\n",
      "epoch [1] loss: 0.687 62.06543967280164 58.19095477386934\n",
      "epoch [2] loss: 0.657 65.439672801636 61.256281407035175\n",
      "epoch [3] loss: 0.633 68.40490797546012 64.87437185929649\n",
      "epoch [4] loss: 0.601 70.44989775051124 65.52763819095478\n",
      "epoch [5] loss: 0.563 73.92638036809817 65.87939698492463\n",
      "epoch [6] loss: 0.535 75.20449897750511 65.52763819095478\n",
      "epoch [7] loss: 0.490 81.95296523517382 66.03015075376885\n",
      "epoch [8] loss: 0.430 81.0838445807771 66.83417085427136\n",
      "epoch [9] loss: 0.389 87.88343558282209 67.98994974874371\n",
      "epoch [10] loss: 0.319 89.2638036809816 68.09045226130654\n",
      "epoch [11] loss: 0.242 93.9161554192229 67.8391959798995\n",
      "epoch [12] loss: 0.174 95.91002044989776 66.13065326633166\n",
      "epoch [13] loss: 0.135 95.5521472392638 66.48241206030151\n",
      "epoch [14] loss: 0.124 97.6482617586912 65.32663316582915\n",
      "epoch [15] loss: 0.059 99.2842535787321 66.98492462311557\n",
      "epoch [16] loss: 0.063 98.46625766871166 66.48241206030151\n",
      "epoch [17] loss: 0.079 99.13087934560328 66.88442211055276\n",
      "epoch [18] loss: 0.024 99.84662576687117 66.93467336683418\n",
      "epoch [19] loss: 0.011 99.74437627811861 67.98994974874371\n",
      "epoch [20] loss: 0.006 100.0 66.38190954773869\n",
      "epoch [21] loss: 0.002 100.0 67.58793969849246\n",
      "epoch [22] loss: 0.001 100.0 67.78894472361809\n",
      "epoch [23] loss: 0.000 100.0 67.73869346733669\n",
      "epoch [24] loss: 0.000 100.0 67.93969849246231\n",
      "epoch [25] loss: 0.000 100.0 67.63819095477388\n",
      "epoch [26] loss: 0.000 100.0 67.53768844221105\n",
      "epoch [27] loss: 0.000 100.0 67.8894472361809\n",
      "epoch [28] loss: 0.000 100.0 67.68844221105527\n",
      "epoch [29] loss: 0.000 100.0 67.78894472361809\n",
      "epoch [30] loss: 0.000 100.0 67.43718592964824\n",
      "Finished Training\n",
      "epoch [1] loss: 0.687 57.43615423134702 56.834170854271356\n",
      "epoch [2] loss: 0.662 66.19929894842264 59.14572864321608\n",
      "epoch [3] loss: 0.628 70.0050075112669 63.21608040201005\n",
      "epoch [4] loss: 0.594 73.00951427140711 63.869346733668344\n",
      "epoch [5] loss: 0.563 75.56334501752629 64.77386934673366\n",
      "epoch [6] loss: 0.528 73.25988983475213 63.81909547738694\n",
      "epoch [7] loss: 0.488 80.97145718577866 65.07537688442211\n",
      "epoch [8] loss: 0.449 84.52679018527792 65.37688442211055\n",
      "epoch [9] loss: 0.379 88.13219829744617 65.678391959799\n",
      "epoch [10] loss: 0.311 91.63745618427642 65.62814070351759\n",
      "epoch [11] loss: 0.265 86.07911867801702 63.768844221105525\n",
      "epoch [12] loss: 0.194 95.54331497245869 64.82412060301507\n",
      "epoch [13] loss: 0.126 98.5978968452679 65.82914572864321\n",
      "epoch [14] loss: 0.077 97.59639459188783 64.62311557788945\n",
      "epoch [15] loss: 0.098 98.34752128192288 63.969849246231156\n",
      "epoch [16] loss: 0.071 97.69654481722584 65.22613065326634\n",
      "epoch [17] loss: 0.054 98.99849774661993 64.42211055276383\n",
      "epoch [18] loss: 0.050 98.14722083124687 63.91959798994975\n",
      "epoch [19] loss: 0.043 99.59939909864798 64.12060301507537\n",
      "epoch [20] loss: 0.010 100.0 65.22613065326634\n",
      "epoch [21] loss: 0.005 100.0 65.17587939698493\n",
      "epoch [22] loss: 0.001 100.0 64.62311557788945\n",
      "epoch [23] loss: 0.001 100.0 64.92462311557789\n",
      "epoch [24] loss: 0.000 100.0 65.27638190954774\n",
      "epoch [25] loss: 0.000 100.0 64.9748743718593\n",
      "epoch [26] loss: 0.000 100.0 64.9748743718593\n",
      "epoch [27] loss: 0.000 100.0 64.77386934673366\n",
      "epoch [28] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [29] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [30] loss: 0.000 100.0 64.87437185929649\n",
      "Finished Training\n",
      "1586387747\n",
      "epoch [1] loss: 0.684 57.464212678936605 54.72361809045226\n",
      "epoch [2] loss: 0.661 61.809815950920246 58.89447236180904\n",
      "epoch [3] loss: 0.631 69.06952965235173 61.05527638190955\n",
      "epoch [4] loss: 0.604 70.29652351738241 63.81909547738694\n",
      "epoch [5] loss: 0.576 74.2842535787321 64.52261306532664\n",
      "epoch [6] loss: 0.542 78.16973415132924 65.92964824120602\n",
      "epoch [7] loss: 0.496 80.47034764826176 65.97989949748744\n",
      "epoch [8] loss: 0.444 84.3558282208589 67.53768844221105\n",
      "epoch [9] loss: 0.393 90.08179959100204 67.28643216080403\n",
      "epoch [10] loss: 0.312 91.05316973415133 65.47738693467336\n",
      "epoch [11] loss: 0.251 91.87116564417178 66.4321608040201\n",
      "epoch [12] loss: 0.187 95.24539877300613 66.83417085427136\n",
      "epoch [13] loss: 0.137 96.98364008179959 64.92462311557789\n",
      "epoch [14] loss: 0.110 98.87525562372188 66.18090452261306\n",
      "epoch [15] loss: 0.045 97.49488752556238 65.22613065326634\n",
      "epoch [16] loss: 0.083 97.54601226993866 63.91959798994975\n",
      "epoch [17] loss: 0.074 99.38650306748467 65.12562814070351\n",
      "epoch [18] loss: 0.027 99.84662576687117 66.28140703517587\n",
      "epoch [19] loss: 0.006 99.94887525562372 67.1859296482412\n",
      "epoch [20] loss: 0.002 100.0 67.23618090452261\n",
      "epoch [21] loss: 0.001 100.0 67.1859296482412\n",
      "epoch [22] loss: 0.001 100.0 67.03517587939699\n",
      "epoch [23] loss: 0.000 100.0 67.08542713567839\n",
      "epoch [24] loss: 0.000 100.0 66.88442211055276\n",
      "epoch [25] loss: 0.000 100.0 66.88442211055276\n",
      "epoch [26] loss: 0.000 100.0 66.88442211055276\n",
      "epoch [27] loss: 0.000 100.0 66.98492462311557\n",
      "epoch [28] loss: 0.000 100.0 66.73366834170854\n",
      "epoch [29] loss: 0.000 100.0 66.93467336683418\n",
      "epoch [30] loss: 0.000 100.0 67.03517587939699\n",
      "Finished Training\n",
      "epoch [1] loss: 0.688 62.39359038557837 55.42713567839196\n",
      "epoch [2] loss: 0.672 63.59539308963445 58.24120603015076\n",
      "epoch [3] loss: 0.643 65.04757135703555 62.01005025125628\n",
      "epoch [4] loss: 0.617 71.10665998998498 63.41708542713568\n",
      "epoch [5] loss: 0.579 71.65748622934402 64.87437185929649\n",
      "epoch [6] loss: 0.544 79.36905358037056 67.08542713567839\n",
      "epoch [7] loss: 0.497 82.37356034051076 65.87939698492463\n",
      "epoch [8] loss: 0.435 83.37506259389083 66.38190954773869\n",
      "epoch [9] loss: 0.390 84.02603905858788 65.57788944723617\n",
      "epoch [10] loss: 0.317 90.08512769153731 65.92964824120602\n",
      "epoch [11] loss: 0.260 94.89233850776164 66.98492462311557\n",
      "epoch [12] loss: 0.197 94.94241362043064 64.2713567839196\n",
      "epoch [13] loss: 0.164 95.29293940911367 66.48241206030151\n",
      "epoch [14] loss: 0.101 98.34752128192288 66.58291457286433\n",
      "epoch [15] loss: 0.100 98.79819729594392 65.22613065326634\n",
      "epoch [16] loss: 0.068 98.89834752128192 65.97989949748744\n",
      "epoch [17] loss: 0.026 99.64947421131697 65.57788944723617\n",
      "epoch [18] loss: 0.028 98.89834752128192 66.73366834170854\n",
      "epoch [19] loss: 0.069 99.19879819729594 65.97989949748744\n",
      "epoch [20] loss: 0.064 99.09864797195793 65.22613065326634\n",
      "epoch [21] loss: 0.019 100.0 65.82914572864321\n",
      "epoch [22] loss: 0.047 97.94692038057086 64.9748743718593\n",
      "epoch [23] loss: 0.090 96.79519278918377 64.07035175879398\n",
      "epoch [24] loss: 0.029 100.0 65.87939698492463\n",
      "epoch [25] loss: 0.004 100.0 65.87939698492463\n",
      "epoch [26] loss: 0.001 100.0 66.53266331658291\n",
      "epoch [27] loss: 0.001 100.0 66.73366834170854\n",
      "epoch [28] loss: 0.000 100.0 66.73366834170854\n",
      "epoch [29] loss: 0.000 100.0 66.48241206030151\n",
      "epoch [30] loss: 0.000 100.0 66.58291457286433\n",
      "Finished Training\n",
      "1586387929\n",
      "epoch [1] loss: 0.692 54.65235173824131 55.37688442211055\n",
      "epoch [2] loss: 0.671 65.95092024539878 62.51256281407035\n",
      "epoch [3] loss: 0.645 67.89366053169734 63.266331658291456\n",
      "epoch [4] loss: 0.609 71.77914110429448 65.678391959799\n",
      "epoch [5] loss: 0.574 71.0122699386503 65.87939698492463\n",
      "epoch [6] loss: 0.560 74.89775051124744 63.36683417085427\n",
      "epoch [7] loss: 0.514 79.95910020449898 65.42713567839196\n",
      "epoch [8] loss: 0.479 82.0040899795501 66.48241206030151\n",
      "epoch [9] loss: 0.421 82.20858895705521 66.48241206030151\n",
      "epoch [10] loss: 0.350 90.1840490797546 66.53266331658291\n",
      "epoch [11] loss: 0.296 89.97955010224949 67.03517587939699\n",
      "epoch [12] loss: 0.198 94.93865030674847 66.83417085427136\n",
      "epoch [13] loss: 0.159 97.08588957055214 66.83417085427136\n",
      "epoch [14] loss: 0.099 96.88139059304703 65.678391959799\n",
      "epoch [15] loss: 0.082 99.2842535787321 65.77889447236181\n",
      "epoch [16] loss: 0.054 98.26175869120654 64.321608040201\n",
      "epoch [17] loss: 0.050 97.39263803680981 65.47738693467336\n",
      "epoch [18] loss: 0.075 98.00613496932516 67.23618090452261\n",
      "epoch [19] loss: 0.072 98.77300613496932 65.12562814070351\n",
      "epoch [20] loss: 0.040 98.51738241308793 65.22613065326634\n",
      "epoch [21] loss: 0.041 99.48875255623722 65.7286432160804\n",
      "epoch [22] loss: 0.052 98.61963190184049 65.92964824120602\n",
      "epoch [23] loss: 0.044 97.6482617586912 64.12060301507537\n",
      "epoch [24] loss: 0.011 99.48875255623722 66.48241206030151\n",
      "epoch [25] loss: 0.016 99.94887525562372 66.63316582914572\n",
      "epoch [26] loss: 0.003 100.0 67.33668341708542\n",
      "epoch [27] loss: 0.001 100.0 66.48241206030151\n",
      "epoch [28] loss: 0.000 100.0 66.4321608040201\n",
      "epoch [29] loss: 0.000 100.0 66.48241206030151\n",
      "epoch [30] loss: 0.000 100.0 66.58291457286433\n",
      "Finished Training\n",
      "epoch [1] loss: 0.686 61.3420130195293 56.130653266331656\n",
      "epoch [2] loss: 0.667 62.99449173760641 57.48743718592965\n",
      "epoch [3] loss: 0.640 68.7030545818728 62.8643216080402\n",
      "epoch [4] loss: 0.608 71.557336004006 64.02010050251256\n",
      "epoch [5] loss: 0.566 73.20981472208312 65.77889447236181\n",
      "epoch [6] loss: 0.518 79.46920380570856 65.92964824120602\n",
      "epoch [7] loss: 0.469 78.5177766649975 64.72361809045226\n",
      "epoch [8] loss: 0.437 84.37656484727091 67.48743718592965\n",
      "epoch [9] loss: 0.360 89.83475212819229 68.89447236180905\n",
      "epoch [10] loss: 0.298 91.4872308462694 64.2713567839196\n",
      "epoch [11] loss: 0.237 94.14121181772659 65.22613065326634\n",
      "epoch [12] loss: 0.172 96.94541812719079 67.33668341708542\n",
      "epoch [13] loss: 0.099 98.74812218327492 67.43718592964824\n",
      "epoch [14] loss: 0.079 98.5478217325989 66.93467336683418\n",
      "epoch [15] loss: 0.060 98.14722083124687 65.97989949748744\n",
      "epoch [16] loss: 0.100 98.39759639459189 66.33165829145729\n",
      "epoch [17] loss: 0.058 98.94842263395093 66.18090452261306\n",
      "epoch [18] loss: 0.038 99.04857285928894 64.72361809045226\n",
      "epoch [19] loss: 0.031 99.34902353530295 65.7286432160804\n",
      "epoch [20] loss: 0.020 99.39909864797195 64.87437185929649\n",
      "epoch [21] loss: 0.027 98.94842263395093 65.7286432160804\n",
      "epoch [22] loss: 0.022 99.79969954932399 65.47738693467336\n",
      "epoch [23] loss: 0.008 99.44917376064096 63.91959798994975\n",
      "epoch [24] loss: 0.109 98.34752128192288 65.82914572864321\n",
      "epoch [25] loss: 0.043 99.79969954932399 65.42713567839196\n",
      "epoch [26] loss: 0.018 99.949924887331 65.17587939698493\n",
      "epoch [27] loss: 0.003 100.0 66.58291457286433\n",
      "epoch [28] loss: 0.001 100.0 66.33165829145729\n",
      "epoch [29] loss: 0.000 100.0 65.92964824120602\n",
      "epoch [30] loss: 0.000 100.0 65.97989949748744\n",
      "Finished Training\n",
      "1586388110\n",
      "epoch [1] loss: 0.689 57.92433537832311 53.66834170854271\n",
      "epoch [2] loss: 0.663 62.78118609406953 60.552763819095475\n",
      "epoch [3] loss: 0.638 64.2638036809816 59.14572864321608\n",
      "epoch [4] loss: 0.609 71.06339468302659 63.46733668341709\n",
      "epoch [5] loss: 0.568 75.61349693251533 64.12060301507537\n",
      "epoch [6] loss: 0.536 76.89161554192229 64.2713567839196\n",
      "epoch [7] loss: 0.469 79.80572597137014 65.77889447236181\n",
      "epoch [8] loss: 0.417 87.57668711656441 67.08542713567839\n",
      "epoch [9] loss: 0.345 89.92842535787321 65.82914572864321\n",
      "epoch [10] loss: 0.310 92.12678936605317 64.22110552763819\n",
      "epoch [11] loss: 0.218 94.58077709611452 66.18090452261306\n",
      "epoch [12] loss: 0.188 94.52965235173824 64.52261306532664\n",
      "epoch [13] loss: 0.106 97.49488752556238 66.63316582914572\n",
      "epoch [14] loss: 0.066 99.079754601227 66.48241206030151\n",
      "epoch [15] loss: 0.075 98.97750511247443 65.07537688442211\n",
      "epoch [16] loss: 0.080 98.87525562372188 66.03015075376885\n",
      "epoch [17] loss: 0.046 99.84662576687117 65.82914572864321\n",
      "epoch [18] loss: 0.045 96.11451942740287 63.618090452261306\n",
      "epoch [19] loss: 0.033 99.13087934560328 65.52763819095478\n",
      "epoch [20] loss: 0.026 98.92638036809817 65.62814070351759\n",
      "epoch [21] loss: 0.017 99.33537832310839 63.969849246231156\n",
      "epoch [22] loss: 0.085 95.60327198364008 64.07035175879398\n",
      "epoch [23] loss: 0.081 99.02862985685071 65.17587939698493\n",
      "epoch [24] loss: 0.015 99.48875255623722 63.768844221105525\n",
      "epoch [25] loss: 0.009 100.0 65.47738693467336\n",
      "epoch [26] loss: 0.002 100.0 65.92964824120602\n",
      "epoch [27] loss: 0.001 100.0 65.678391959799\n",
      "epoch [28] loss: 0.000 100.0 65.82914572864321\n",
      "epoch [29] loss: 0.000 100.0 66.03015075376885\n",
      "epoch [30] loss: 0.000 100.0 66.13065326633166\n",
      "Finished Training\n",
      "epoch [1] loss: 0.689 55.58337506259389 55.42713567839196\n",
      "epoch [2] loss: 0.671 64.74712068102153 59.19597989949749\n",
      "epoch [3] loss: 0.627 68.90335503254882 59.74874371859296\n",
      "epoch [4] loss: 0.590 72.8092138207311 63.869346733668344\n",
      "epoch [5] loss: 0.549 75.8137205808713 63.015075376884425\n",
      "epoch [6] loss: 0.516 77.96695042563846 62.914572864321606\n",
      "epoch [7] loss: 0.471 81.02153229844767 63.869346733668344\n",
      "epoch [8] loss: 0.417 82.17325988983475 64.82412060301507\n",
      "epoch [9] loss: 0.353 89.13370055082623 65.82914572864321\n",
      "epoch [10] loss: 0.283 89.68452679018527 65.92964824120602\n",
      "epoch [11] loss: 0.197 94.2914371557336 65.678391959799\n",
      "epoch [12] loss: 0.154 97.2458688032048 64.22110552763819\n",
      "epoch [13] loss: 0.103 96.64496745117677 63.91959798994975\n",
      "epoch [14] loss: 0.101 96.39459188783175 63.36683417085427\n",
      "epoch [15] loss: 0.081 98.84827240861291 63.46733668341709\n",
      "epoch [16] loss: 0.039 99.19879819729594 63.36683417085427\n",
      "epoch [17] loss: 0.017 99.74962443665498 64.12060301507537\n",
      "epoch [18] loss: 0.016 99.64947421131697 63.66834170854271\n",
      "epoch [19] loss: 0.008 100.0 64.72361809045226\n",
      "epoch [20] loss: 0.002 100.0 64.52261306532664\n",
      "epoch [21] loss: 0.001 100.0 64.57286432160804\n",
      "epoch [22] loss: 0.000 100.0 64.52261306532664\n",
      "epoch [23] loss: 0.000 100.0 64.72361809045226\n",
      "epoch [24] loss: 0.000 100.0 64.72361809045226\n",
      "epoch [25] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [26] loss: 0.000 100.0 64.72361809045226\n",
      "epoch [27] loss: 0.000 100.0 64.62311557788945\n",
      "epoch [28] loss: 0.000 100.0 64.87437185929649\n",
      "epoch [29] loss: 0.000 100.0 64.77386934673366\n",
      "epoch [30] loss: 0.000 100.0 64.87437185929649\n",
      "Finished Training\n",
      "1586388289\n",
      "epoch [1] loss: 0.693 61.758691206543965 58.84422110552764\n",
      "epoch [2] loss: 0.678 58.38445807770961 54.2713567839196\n",
      "epoch [3] loss: 0.647 67.58691206543968 63.869346733668344\n",
      "epoch [4] loss: 0.622 67.9959100204499 62.21105527638191\n",
      "epoch [5] loss: 0.597 70.34764826175869 63.36683417085427\n",
      "epoch [6] loss: 0.568 74.48875255623722 65.12562814070351\n",
      "epoch [7] loss: 0.535 77.65848670756647 65.17587939698493\n",
      "epoch [8] loss: 0.513 78.7321063394683 64.57286432160804\n",
      "epoch [9] loss: 0.476 78.93660531697341 63.36683417085427\n",
      "epoch [10] loss: 0.439 82.719836400818 65.42713567839196\n",
      "epoch [11] loss: 0.388 86.65644171779141 64.321608040201\n",
      "epoch [12] loss: 0.324 89.4683026584867 64.62311557788945\n",
      "epoch [13] loss: 0.281 93.35378323108384 65.32663316582915\n",
      "epoch [14] loss: 0.213 93.4560327198364 63.81909547738694\n",
      "epoch [15] loss: 0.159 91.87116564417178 62.71356783919598\n",
      "epoch [16] loss: 0.126 97.23926380368098 66.83417085427136\n",
      "epoch [17] loss: 0.098 98.72188139059304 64.77386934673366\n",
      "epoch [18] loss: 0.079 97.29038854805727 65.07537688442211\n",
      "epoch [19] loss: 0.065 99.18200408997956 63.869346733668344\n",
      "epoch [20] loss: 0.034 99.64212678936606 66.08040201005025\n",
      "epoch [21] loss: 0.053 98.8241308793456 63.618090452261306\n",
      "epoch [22] loss: 0.020 99.48875255623722 64.87437185929649\n",
      "epoch [23] loss: 0.022 99.69325153374233 65.47738693467336\n",
      "epoch [24] loss: 0.009 99.74437627811861 64.42211055276383\n",
      "epoch [25] loss: 0.041 99.79550102249489 65.17587939698493\n",
      "epoch [26] loss: 0.005 100.0 64.47236180904522\n",
      "epoch [27] loss: 0.001 100.0 65.678391959799\n",
      "epoch [28] loss: 0.001 100.0 65.7286432160804\n",
      "epoch [29] loss: 0.001 100.0 65.47738693467336\n",
      "epoch [30] loss: 0.000 100.0 65.32663316582915\n",
      "Finished Training\n",
      "epoch [1] loss: 0.687 61.64246369554331 57.185929648241206\n",
      "epoch [2] loss: 0.660 59.03855783675513 57.085427135678394\n",
      "epoch [3] loss: 0.637 68.10215322984477 64.87437185929649\n",
      "epoch [4] loss: 0.609 70.30545818728092 65.52763819095478\n",
      "epoch [5] loss: 0.575 74.1111667501252 65.07537688442211\n",
      "epoch [6] loss: 0.546 75.6134201301953 66.18090452261306\n",
      "epoch [7] loss: 0.502 80.47070605908863 68.79396984924622\n",
      "epoch [8] loss: 0.447 84.12618928392588 66.28140703517587\n",
      "epoch [9] loss: 0.393 88.13219829744617 66.08040201005025\n",
      "epoch [10] loss: 0.326 87.08062093139709 63.71859296482412\n",
      "epoch [11] loss: 0.262 94.2914371557336 66.93467336683418\n",
      "epoch [12] loss: 0.209 93.08963445167751 67.1356783919598\n",
      "epoch [13] loss: 0.150 94.54181271907862 66.4321608040201\n",
      "epoch [14] loss: 0.113 97.0956434651978 65.27638190954774\n",
      "epoch [15] loss: 0.087 98.29744616925387 65.57788944723617\n",
      "epoch [16] loss: 0.074 98.39759639459189 65.22613065326634\n",
      "epoch [17] loss: 0.078 98.29744616925387 65.52763819095478\n",
      "epoch [18] loss: 0.029 99.29894842263396 65.52763819095478\n",
      "epoch [19] loss: 0.022 99.09864797195793 67.28643216080403\n",
      "epoch [20] loss: 0.013 99.89984977466199 66.28140703517587\n",
      "epoch [21] loss: 0.010 99.949924887331 66.63316582914572\n",
      "epoch [22] loss: 0.071 97.44616925388083 67.28643216080403\n",
      "epoch [23] loss: 0.067 98.84827240861291 66.18090452261306\n",
      "epoch [24] loss: 0.038 99.74962443665498 66.23115577889448\n",
      "epoch [25] loss: 0.027 99.29894842263396 64.57286432160804\n",
      "epoch [26] loss: 0.028 99.79969954932399 66.13065326633166\n",
      "epoch [27] loss: 0.008 100.0 66.48241206030151\n",
      "epoch [28] loss: 0.001 100.0 67.08542713567839\n",
      "epoch [29] loss: 0.001 100.0 67.53768844221105\n",
      "epoch [30] loss: 0.000 100.0 67.63819095477388\n",
      "Finished Training\n",
      "1586388468\n",
      "epoch [1] loss: 0.686 63.394683026584865 58.743718592964825\n",
      "epoch [2] loss: 0.653 65.84867075664621 61.15577889447236\n",
      "epoch [3] loss: 0.619 69.93865030674847 64.52261306532664\n",
      "epoch [4] loss: 0.582 73.46625766871166 64.57286432160804\n",
      "epoch [5] loss: 0.557 78.2719836400818 66.53266331658291\n",
      "epoch [6] loss: 0.504 80.41922290388548 69.14572864321607\n",
      "epoch [7] loss: 0.464 81.28834355828221 67.8894472361809\n",
      "epoch [8] loss: 0.410 86.04294478527608 67.73869346733669\n",
      "epoch [9] loss: 0.340 89.05930470347649 67.23618090452261\n",
      "epoch [10] loss: 0.287 90.84867075664621 67.43718592964824\n",
      "epoch [11] loss: 0.216 94.47852760736197 67.8391959798995\n",
      "epoch [12] loss: 0.168 95.70552147239263 66.73366834170854\n",
      "epoch [13] loss: 0.115 97.69938650306749 66.38190954773869\n",
      "epoch [14] loss: 0.093 97.85276073619632 66.48241206030151\n",
      "epoch [15] loss: 0.106 98.3640081799591 67.78894472361809\n",
      "epoch [16] loss: 0.081 98.92638036809817 67.1859296482412\n",
      "epoch [17] loss: 0.043 98.56850715746421 65.82914572864321\n",
      "epoch [18] loss: 0.057 99.64212678936606 64.82412060301507\n",
      "epoch [19] loss: 0.009 100.0 67.1859296482412\n",
      "epoch [20] loss: 0.002 100.0 67.28643216080403\n",
      "epoch [21] loss: 0.001 100.0 67.48743718592965\n",
      "epoch [22] loss: 0.001 100.0 67.48743718592965\n",
      "epoch [23] loss: 0.000 100.0 67.58793969849246\n",
      "epoch [24] loss: 0.000 100.0 67.73869346733669\n",
      "epoch [25] loss: 0.000 100.0 67.63819095477388\n",
      "epoch [26] loss: 0.000 100.0 68.09045226130654\n",
      "epoch [27] loss: 0.000 100.0 68.09045226130654\n",
      "epoch [28] loss: 0.000 100.0 67.98994974874371\n",
      "epoch [29] loss: 0.000 100.0 67.93969849246231\n",
      "epoch [30] loss: 0.000 100.0 67.93969849246231\n",
      "Finished Training\n",
      "epoch [1] loss: 0.686 61.4922383575363 57.1356783919598\n",
      "epoch [2] loss: 0.659 67.2008012018027 63.66834170854271\n",
      "epoch [3] loss: 0.626 71.70756134201302 65.92964824120602\n",
      "epoch [4] loss: 0.579 73.86079118678018 67.1356783919598\n",
      "epoch [5] loss: 0.550 76.41462193289935 67.8894472361809\n",
      "epoch [6] loss: 0.493 79.51927891837757 66.28140703517587\n",
      "epoch [7] loss: 0.472 76.66499749624437 64.72361809045226\n",
      "epoch [8] loss: 0.400 87.58137205808713 66.83417085427136\n",
      "epoch [9] loss: 0.359 85.22784176264396 66.08040201005025\n",
      "epoch [10] loss: 0.276 88.88332498748122 64.37185929648241\n",
      "epoch [11] loss: 0.229 94.89233850776164 66.08040201005025\n",
      "epoch [12] loss: 0.142 97.2959439158738 65.97989949748744\n",
      "epoch [13] loss: 0.099 97.69654481722584 64.87437185929649\n",
      "epoch [14] loss: 0.078 96.29444166249374 64.47236180904522\n",
      "epoch [15] loss: 0.059 98.74812218327492 66.03015075376885\n",
      "epoch [16] loss: 0.094 98.6479719579369 66.28140703517587\n",
      "epoch [17] loss: 0.032 97.59639459188783 65.27638190954774\n",
      "epoch [18] loss: 0.057 98.5478217325989 64.92462311557789\n",
      "epoch [19] loss: 0.037 99.24887330996495 65.0251256281407\n",
      "epoch [20] loss: 0.044 98.34752128192288 65.17587939698493\n",
      "epoch [21] loss: 0.031 99.54932398597897 65.57788944723617\n",
      "epoch [22] loss: 0.007 100.0 67.1859296482412\n",
      "epoch [23] loss: 0.001 100.0 66.93467336683418\n",
      "epoch [24] loss: 0.001 100.0 67.08542713567839\n",
      "epoch [25] loss: 0.000 100.0 66.73366834170854\n",
      "epoch [26] loss: 0.000 100.0 66.78391959798995\n",
      "epoch [27] loss: 0.000 100.0 66.63316582914572\n",
      "epoch [28] loss: 0.000 100.0 66.68341708542714\n",
      "epoch [29] loss: 0.000 100.0 66.68341708542714\n",
      "epoch [30] loss: 0.000 100.0 66.48241206030151\n",
      "Finished Training\n",
      "1586388648\n",
      "epoch [1] loss: 0.686 61.758691206543965 58.14070351758794\n",
      "epoch [2] loss: 0.660 62.93456032719836 58.743718592964825\n",
      "epoch [3] loss: 0.633 69.58077709611452 62.562814070351756\n",
      "epoch [4] loss: 0.595 73.8241308793456 65.678391959799\n",
      "epoch [5] loss: 0.564 75.97137014314929 65.17587939698493\n",
      "epoch [6] loss: 0.536 73.67075664621677 63.31658291457286\n",
      "epoch [7] loss: 0.479 83.43558282208589 65.87939698492463\n",
      "epoch [8] loss: 0.409 85.32719836400818 66.18090452261306\n",
      "epoch [9] loss: 0.354 86.09406952965236 65.57788944723617\n",
      "epoch [10] loss: 0.306 90.03067484662577 64.62311557788945\n",
      "epoch [11] loss: 0.236 94.1717791411043 66.68341708542714\n",
      "epoch [12] loss: 0.177 96.62576687116564 65.82914572864321\n",
      "epoch [13] loss: 0.127 96.77914110429448 65.7286432160804\n",
      "epoch [14] loss: 0.116 97.54601226993866 63.869346733668344\n",
      "epoch [15] loss: 0.082 96.67689161554192 63.5678391959799\n",
      "epoch [16] loss: 0.040 99.84662576687117 65.32663316582915\n",
      "epoch [17] loss: 0.033 95.8077709611452 65.27638190954774\n",
      "epoch [18] loss: 0.127 98.3640081799591 62.61306532663317\n",
      "epoch [19] loss: 0.044 98.15950920245399 63.5678391959799\n",
      "epoch [20] loss: 0.037 99.5398773006135 65.52763819095478\n",
      "epoch [21] loss: 0.020 99.59100204498978 65.12562814070351\n",
      "epoch [22] loss: 0.033 98.97750511247443 65.22613065326634\n",
      "epoch [23] loss: 0.027 99.43762781186093 64.12060301507537\n",
      "epoch [24] loss: 0.010 99.64212678936606 64.52261306532664\n",
      "epoch [25] loss: 0.007 100.0 65.42713567839196\n",
      "epoch [26] loss: 0.001 100.0 64.92462311557789\n",
      "epoch [27] loss: 0.000 100.0 64.47236180904522\n",
      "epoch [28] loss: 0.000 100.0 64.62311557788945\n",
      "epoch [29] loss: 0.000 100.0 64.87437185929649\n",
      "epoch [30] loss: 0.000 100.0 64.72361809045226\n",
      "Finished Training\n",
      "epoch [1] loss: 0.688 54.03104656985478 51.959798994974875\n",
      "epoch [2] loss: 0.662 65.59839759639459 58.19095477386934\n",
      "epoch [3] loss: 0.631 70.0050075112669 63.5678391959799\n",
      "epoch [4] loss: 0.587 74.06109163745619 65.42713567839196\n",
      "epoch [5] loss: 0.541 74.16124186279418 63.66834170854271\n",
      "epoch [6] loss: 0.509 77.1657486229344 65.97989949748744\n",
      "epoch [7] loss: 0.472 82.07310966449674 65.22613065326634\n",
      "epoch [8] loss: 0.429 82.67401101652479 64.92462311557789\n",
      "epoch [9] loss: 0.371 89.28392588883325 68.69346733668341\n",
      "epoch [10] loss: 0.297 93.18978467701552 67.43718592964824\n",
      "epoch [11] loss: 0.250 91.73760640961441 64.57286432160804\n",
      "epoch [12] loss: 0.172 98.09714571857786 67.23618090452261\n",
      "epoch [13] loss: 0.112 98.39759639459189 66.63316582914572\n",
      "epoch [14] loss: 0.086 98.09714571857786 66.73366834170854\n",
      "epoch [15] loss: 0.085 98.5978968452679 65.37688442211055\n",
      "epoch [16] loss: 0.041 98.99849774661993 66.53266331658291\n",
      "epoch [17] loss: 0.056 98.44767150726089 66.58291457286433\n",
      "epoch [18] loss: 0.058 98.29744616925387 66.23115577889448\n",
      "epoch [19] loss: 0.046 97.99699549323987 65.17587939698493\n",
      "epoch [20] loss: 0.052 99.74962443665498 65.57788944723617\n",
      "epoch [21] loss: 0.015 99.04857285928894 66.03015075376885\n",
      "epoch [22] loss: 0.032 99.04857285928894 65.32663316582915\n",
      "epoch [23] loss: 0.013 99.89984977466199 65.52763819095478\n",
      "epoch [24] loss: 0.003 99.849774661993 66.53266331658291\n",
      "epoch [25] loss: 0.048 98.84827240861291 65.07537688442211\n",
      "epoch [26] loss: 0.053 99.69954932398598 66.78391959798995\n",
      "epoch [27] loss: 0.012 99.89984977466199 66.63316582914572\n",
      "epoch [28] loss: 0.015 99.79969954932399 67.23618090452261\n",
      "epoch [29] loss: 0.006 99.89984977466199 65.62814070351759\n",
      "epoch [30] loss: 0.033 99.59939909864798 66.78391959798995\n",
      "Finished Training\n",
      "1586388830\n",
      "epoch [1] loss: 0.683 64.87730061349693 60.80402010050251\n",
      "epoch [2] loss: 0.671 63.394683026584865 60.402010050251256\n",
      "epoch [3] loss: 0.623 68.96728016359918 63.165829145728644\n",
      "epoch [4] loss: 0.596 72.39263803680981 65.62814070351759\n",
      "epoch [5] loss: 0.570 73.41513292433538 64.62311557788945\n",
      "epoch [6] loss: 0.525 76.84049079754601 66.33165829145729\n",
      "epoch [7] loss: 0.482 82.41308793456032 67.08542713567839\n",
      "epoch [8] loss: 0.420 87.21881390593047 67.23618090452261\n",
      "epoch [9] loss: 0.356 90.23517382413088 67.1356783919598\n",
      "epoch [10] loss: 0.281 90.95092024539878 67.38693467336684\n",
      "epoch [11] loss: 0.218 94.98977505112474 64.9748743718593\n",
      "epoch [12] loss: 0.172 97.34151329243353 66.98492462311557\n",
      "epoch [13] loss: 0.111 98.26175869120654 65.17587939698493\n",
      "epoch [14] loss: 0.076 98.26175869120654 65.37688442211055\n",
      "epoch [15] loss: 0.066 99.48875255623722 65.57788944723617\n",
      "epoch [16] loss: 0.035 99.48875255623722 66.93467336683418\n",
      "epoch [17] loss: 0.027 99.02862985685071 65.27638190954774\n",
      "epoch [18] loss: 0.051 98.41513292433538 64.62311557788945\n",
      "epoch [19] loss: 0.048 98.77300613496932 65.42713567839196\n",
      "epoch [20] loss: 0.074 99.33537832310839 66.18090452261306\n",
      "epoch [21] loss: 0.024 99.94887525562372 65.22613065326634\n",
      "epoch [22] loss: 0.004 100.0 65.27638190954774\n",
      "epoch [23] loss: 0.001 100.0 65.22613065326634\n",
      "epoch [24] loss: 0.001 100.0 65.32663316582915\n",
      "epoch [25] loss: 0.000 100.0 65.22613065326634\n",
      "epoch [26] loss: 0.000 100.0 65.37688442211055\n",
      "epoch [27] loss: 0.000 100.0 65.57788944723617\n",
      "epoch [28] loss: 0.000 100.0 65.57788944723617\n",
      "epoch [29] loss: 0.000 100.0 65.47738693467336\n",
      "epoch [30] loss: 0.000 100.0 65.42713567839196\n",
      "Finished Training\n",
      "epoch [1] loss: 0.684 60.44066099148723 55.27638190954774\n",
      "epoch [2] loss: 0.655 66.64997496244366 61.85929648241206\n",
      "epoch [3] loss: 0.617 67.65147721582373 62.311557788944725\n",
      "epoch [4] loss: 0.578 74.61191787681523 65.62814070351759\n",
      "epoch [5] loss: 0.544 77.41612418627942 64.92462311557789\n",
      "epoch [6] loss: 0.500 78.06710065097647 64.22110552763819\n",
      "epoch [7] loss: 0.460 81.5222834251377 67.53768844221105\n",
      "epoch [8] loss: 0.402 87.93189784677016 68.04020100502512\n",
      "epoch [9] loss: 0.335 89.73460190285428 64.87437185929649\n",
      "epoch [10] loss: 0.263 91.88783174762143 66.08040201005025\n",
      "epoch [11] loss: 0.200 93.49023535302955 66.23115577889448\n",
      "epoch [12] loss: 0.150 97.34601902854281 66.83417085427136\n",
      "epoch [13] loss: 0.107 97.89684526790185 66.93467336683418\n",
      "epoch [14] loss: 0.073 98.6479719579369 66.28140703517587\n",
      "epoch [15] loss: 0.081 98.29744616925387 66.78391959798995\n",
      "epoch [16] loss: 0.039 99.79969954932399 65.92964824120602\n",
      "epoch [17] loss: 0.036 97.49624436654983 64.87437185929649\n",
      "epoch [18] loss: 0.045 99.59939909864798 67.1356783919598\n",
      "epoch [19] loss: 0.011 100.0 66.53266331658291\n",
      "epoch [20] loss: 0.003 100.0 67.08542713567839\n",
      "epoch [21] loss: 0.001 100.0 67.28643216080403\n",
      "epoch [22] loss: 0.001 100.0 67.43718592964824\n",
      "epoch [23] loss: 0.000 100.0 67.38693467336684\n",
      "epoch [24] loss: 0.000 100.0 67.28643216080403\n",
      "epoch [25] loss: 0.000 100.0 67.03517587939699\n",
      "epoch [26] loss: 0.000 100.0 67.28643216080403\n",
      "epoch [27] loss: 0.000 100.0 66.98492462311557\n",
      "epoch [28] loss: 0.000 100.0 66.98492462311557\n",
      "epoch [29] loss: 0.000 100.0 67.08542713567839\n",
      "epoch [30] loss: 0.000 100.0 67.08542713567839\n",
      "Finished Training\n",
      "1586389010\n",
      "epoch [1] loss: 0.683 58.84458077709611 56.482412060301506\n",
      "epoch [2] loss: 0.651 66.97341513292433 62.36180904522613\n",
      "epoch [3] loss: 0.626 66.00204498977506 62.311557788944725\n",
      "epoch [4] loss: 0.601 73.41513292433538 66.03015075376885\n",
      "epoch [5] loss: 0.563 76.02249488752557 67.1356783919598\n",
      "epoch [6] loss: 0.527 79.08997955010224 67.73869346733669\n",
      "epoch [7] loss: 0.479 79.14110429447852 64.42211055276383\n",
      "epoch [8] loss: 0.421 83.94683026584867 68.94472361809045\n",
      "epoch [9] loss: 0.363 87.06543967280163 68.49246231155779\n",
      "epoch [10] loss: 0.301 91.1042944785276 66.53266331658291\n",
      "epoch [11] loss: 0.225 93.96728016359918 68.5427135678392\n",
      "epoch [12] loss: 0.175 95.04089979550102 66.93467336683418\n",
      "epoch [13] loss: 0.162 96.98364008179959 68.79396984924622\n",
      "epoch [14] loss: 0.083 98.51738241308793 66.98492462311557\n",
      "epoch [15] loss: 0.056 99.13087934560328 67.73869346733669\n",
      "epoch [16] loss: 0.084 97.4437627811861 65.92964824120602\n",
      "epoch [17] loss: 0.042 98.92638036809817 67.78894472361809\n",
      "epoch [18] loss: 0.067 97.80163599182004 67.48743718592965\n",
      "epoch [19] loss: 0.047 97.34151329243353 65.32663316582915\n",
      "epoch [20] loss: 0.043 99.2842535787321 67.53768844221105\n",
      "epoch [21] loss: 0.026 99.64212678936606 66.78391959798995\n",
      "epoch [22] loss: 0.042 98.8241308793456 66.68341708542714\n",
      "epoch [23] loss: 0.027 99.64212678936606 66.28140703517587\n",
      "epoch [24] loss: 0.019 99.23312883435582 68.19095477386935\n",
      "epoch [25] loss: 0.025 99.2842535787321 66.63316582914572\n",
      "epoch [26] loss: 0.029 99.02862985685071 67.33668341708542\n",
      "epoch [27] loss: 0.065 99.38650306748467 66.93467336683418\n",
      "epoch [28] loss: 0.014 99.84662576687117 66.4321608040201\n",
      "epoch [29] loss: 0.004 100.0 66.98492462311557\n",
      "epoch [30] loss: 0.001 100.0 67.28643216080403\n",
      "Finished Training\n",
      "epoch [1] loss: 0.689 62.8442663995994 56.28140703517588\n",
      "epoch [2] loss: 0.664 66.85027541311968 62.663316582914575\n",
      "epoch [3] loss: 0.635 70.20530796194292 64.42211055276383\n",
      "epoch [4] loss: 0.596 72.9594391587381 65.77889447236181\n",
      "epoch [5] loss: 0.558 75.6134201301953 66.23115577889448\n",
      "epoch [6] loss: 0.516 72.60891337005508 64.321608040201\n",
      "epoch [7] loss: 0.491 80.62093139709565 66.58291457286433\n",
      "epoch [8] loss: 0.425 86.47971957936906 67.78894472361809\n",
      "epoch [9] loss: 0.377 86.07911867801702 64.82412060301507\n",
      "epoch [10] loss: 0.309 90.53580370555834 66.58291457286433\n",
      "epoch [11] loss: 0.244 94.04106159238859 65.17587939698493\n",
      "epoch [12] loss: 0.199 96.54481722583876 65.07537688442211\n",
      "epoch [13] loss: 0.139 95.94391587381071 65.92964824120602\n",
      "epoch [14] loss: 0.095 97.34601902854281 64.92462311557789\n",
      "epoch [15] loss: 0.093 98.6479719579369 64.321608040201\n",
      "epoch [16] loss: 0.054 98.84827240861291 66.63316582914572\n",
      "epoch [17] loss: 0.057 99.14872308462694 65.32663316582915\n",
      "epoch [18] loss: 0.074 97.39609414121182 64.2713567839196\n",
      "epoch [19] loss: 0.046 99.09864797195793 64.87437185929649\n",
      "epoch [20] loss: 0.040 98.29744616925387 65.27638190954774\n",
      "epoch [21] loss: 0.037 98.89834752128192 67.08542713567839\n",
      "epoch [22] loss: 0.029 99.04857285928894 65.52763819095478\n",
      "epoch [23] loss: 0.042 98.84827240861291 65.57788944723617\n",
      "epoch [24] loss: 0.038 99.49924887330997 65.52763819095478\n",
      "epoch [25] loss: 0.010 100.0 66.98492462311557\n",
      "epoch [26] loss: 0.002 100.0 66.73366834170854\n",
      "epoch [27] loss: 0.001 100.0 66.88442211055276\n",
      "epoch [28] loss: 0.000 100.0 66.78391959798995\n",
      "epoch [29] loss: 0.000 100.0 67.03517587939699\n",
      "epoch [30] loss: 0.000 100.0 66.98492462311557\n",
      "Finished Training\n",
      "1586389190\n",
      "epoch [1] loss: 0.687 61.29856850715746 55.92964824120603\n",
      "epoch [2] loss: 0.663 64.2638036809816 61.4070351758794\n",
      "epoch [3] loss: 0.632 63.59918200408998 59.39698492462311\n",
      "epoch [4] loss: 0.603 70.34764826175869 63.969849246231156\n",
      "epoch [5] loss: 0.579 73.77300613496932 65.92964824120602\n",
      "epoch [6] loss: 0.546 74.2842535787321 66.23115577889448\n",
      "epoch [7] loss: 0.517 75.15337423312883 65.82914572864321\n",
      "epoch [8] loss: 0.466 82.10633946830266 67.93969849246231\n",
      "epoch [9] loss: 0.432 85.42944785276073 67.33668341708542\n",
      "epoch [10] loss: 0.372 90.03067484662577 65.678391959799\n",
      "epoch [11] loss: 0.302 92.68916155419222 67.68844221105527\n",
      "epoch [12] loss: 0.259 91.87116564417178 66.03015075376885\n",
      "epoch [13] loss: 0.196 91.8200408997955 66.48241206030151\n",
      "epoch [14] loss: 0.139 95.5521472392638 67.53768844221105\n",
      "epoch [15] loss: 0.095 93.96728016359918 64.57286432160804\n",
      "epoch [16] loss: 0.091 98.00613496932516 65.678391959799\n",
      "epoch [17] loss: 0.046 99.59100204498978 64.47236180904522\n",
      "epoch [18] loss: 0.111 98.72188139059304 65.82914572864321\n",
      "epoch [19] loss: 0.061 97.23926380368098 64.52261306532664\n",
      "epoch [20] loss: 0.027 99.94887525562372 66.23115577889448\n",
      "epoch [21] loss: 0.005 100.0 65.42713567839196\n",
      "epoch [22] loss: 0.001 100.0 66.08040201005025\n",
      "epoch [23] loss: 0.001 100.0 65.87939698492463\n",
      "epoch [24] loss: 0.001 100.0 65.82914572864321\n",
      "epoch [25] loss: 0.000 100.0 65.87939698492463\n",
      "epoch [26] loss: 0.000 100.0 65.97989949748744\n",
      "epoch [27] loss: 0.000 100.0 65.87939698492463\n",
      "epoch [28] loss: 0.000 100.0 65.92964824120602\n",
      "epoch [29] loss: 0.000 100.0 65.82914572864321\n",
      "epoch [30] loss: 0.000 100.0 65.82914572864321\n",
      "Finished Training\n",
      "epoch [1] loss: 0.685 56.184276414621934 54.17085427135678\n",
      "epoch [2] loss: 0.662 66.54982473710565 63.266331658291456\n",
      "epoch [3] loss: 0.630 70.0550826239359 63.266331658291456\n",
      "epoch [4] loss: 0.590 73.51026539809715 64.47236180904522\n",
      "epoch [5] loss: 0.554 77.1657486229344 66.4321608040201\n",
      "epoch [6] loss: 0.522 78.86830245368053 68.84422110552764\n",
      "epoch [7] loss: 0.492 75.26289434151226 63.165829145728644\n",
      "epoch [8] loss: 0.433 85.17776664997496 66.88442211055276\n",
      "epoch [9] loss: 0.362 84.97746619929895 64.22110552763819\n",
      "epoch [10] loss: 0.291 91.28693039559339 65.57788944723617\n",
      "epoch [11] loss: 0.234 93.08963445167751 65.97989949748744\n",
      "epoch [12] loss: 0.177 96.09414121181773 67.33668341708542\n",
      "epoch [13] loss: 0.134 96.99549323985978 66.98492462311557\n",
      "epoch [14] loss: 0.098 98.29744616925387 64.67336683417085\n",
      "epoch [15] loss: 0.069 98.19729594391588 65.62814070351759\n",
      "epoch [16] loss: 0.134 95.99399098647972 65.57788944723617\n",
      "epoch [17] loss: 0.052 99.69954932398598 65.77889447236181\n",
      "epoch [18] loss: 0.024 99.74962443665498 66.4321608040201\n",
      "epoch [19] loss: 0.022 98.44767150726089 65.07537688442211\n",
      "epoch [20] loss: 0.055 99.24887330996495 65.47738693467336\n",
      "epoch [21] loss: 0.056 98.29744616925387 66.73366834170854\n",
      "epoch [22] loss: 0.058 98.6479719579369 64.67336683417085\n",
      "epoch [23] loss: 0.029 99.59939909864798 66.08040201005025\n",
      "epoch [24] loss: 0.021 99.74962443665498 65.17587939698493\n",
      "epoch [25] loss: 0.023 99.24887330996495 66.4321608040201\n",
      "epoch [26] loss: 0.045 99.69954932398598 64.42211055276383\n",
      "epoch [27] loss: 0.019 99.79969954932399 66.4321608040201\n",
      "epoch [28] loss: 0.011 99.79969954932399 64.17085427135679\n",
      "epoch [29] loss: 0.034 99.19879819729594 64.92462311557789\n",
      "epoch [30] loss: 0.020 99.89984977466199 65.77889447236181\n",
      "Finished Training\n",
      "1586389370\n",
      "epoch [1] loss: 0.684 61.24744376278119 58.391959798994975\n",
      "epoch [2] loss: 0.667 65.33742331288343 63.11557788944724\n",
      "epoch [3] loss: 0.636 69.12065439672801 65.32663316582915\n",
      "epoch [4] loss: 0.603 71.31901840490798 63.768844221105525\n",
      "epoch [5] loss: 0.578 73.10838445807771 62.562814070351756\n",
      "epoch [6] loss: 0.535 78.2719836400818 65.32663316582915\n",
      "epoch [7] loss: 0.483 81.28834355828221 64.72361809045226\n",
      "epoch [8] loss: 0.435 84.45807770961146 64.82412060301507\n",
      "epoch [9] loss: 0.382 86.50306748466258 63.768844221105525\n",
      "epoch [10] loss: 0.307 91.56441717791411 64.82412060301507\n",
      "epoch [11] loss: 0.257 90.74642126789367 63.266331658291456\n",
      "epoch [12] loss: 0.180 94.78527607361963 65.12562814070351\n",
      "epoch [13] loss: 0.136 97.03476482617587 65.32663316582915\n",
      "epoch [14] loss: 0.097 98.72188139059304 63.91959798994975\n",
      "epoch [15] loss: 0.094 96.67689161554192 63.06532663316583\n",
      "epoch [16] loss: 0.079 98.05725971370143 64.07035175879398\n",
      "epoch [17] loss: 0.092 97.34151329243353 63.768844221105525\n",
      "epoch [18] loss: 0.048 99.94887525562372 64.12060301507537\n",
      "epoch [19] loss: 0.011 99.94887525562372 64.22110552763819\n",
      "epoch [20] loss: 0.004 100.0 63.969849246231156\n",
      "epoch [21] loss: 0.001 100.0 64.12060301507537\n",
      "epoch [22] loss: 0.001 100.0 64.17085427135679\n",
      "epoch [23] loss: 0.000 100.0 64.02010050251256\n",
      "epoch [24] loss: 0.000 100.0 64.17085427135679\n",
      "epoch [25] loss: 0.000 100.0 64.22110552763819\n",
      "epoch [26] loss: 0.000 100.0 64.12060301507537\n",
      "epoch [27] loss: 0.000 100.0 63.91959798994975\n",
      "epoch [28] loss: 0.000 100.0 64.07035175879398\n",
      "epoch [29] loss: 0.000 100.0 64.07035175879398\n",
      "epoch [30] loss: 0.000 100.0 64.17085427135679\n",
      "Finished Training\n",
      "epoch [1] loss: 0.688 60.090135202804206 57.085427135678394\n",
      "epoch [2] loss: 0.669 63.49524286429644 61.80904522613066\n",
      "epoch [3] loss: 0.641 66.24937406109164 61.80904522613066\n",
      "epoch [4] loss: 0.611 69.50425638457686 63.66834170854271\n",
      "epoch [5] loss: 0.574 72.55883825738609 63.266331658291456\n",
      "epoch [6] loss: 0.543 74.91236855282925 63.91959798994975\n",
      "epoch [7] loss: 0.494 77.1657486229344 64.22110552763819\n",
      "epoch [8] loss: 0.456 85.52829243865799 66.63316582914572\n",
      "epoch [9] loss: 0.370 88.5327991987982 66.28140703517587\n",
      "epoch [10] loss: 0.321 91.28693039559339 67.28643216080403\n",
      "epoch [11] loss: 0.249 93.74061091637456 66.53266331658291\n",
      "epoch [12] loss: 0.171 96.94541812719079 64.17085427135679\n",
      "epoch [13] loss: 0.137 98.99849774661993 64.37185929648241\n",
      "epoch [14] loss: 0.066 98.14722083124687 63.015075376884425\n",
      "epoch [15] loss: 0.078 97.94692038057086 62.71356783919598\n",
      "epoch [16] loss: 0.065 98.69804707060591 62.663316582914575\n",
      "epoch [17] loss: 0.055 99.29894842263396 63.517587939698494\n",
      "epoch [18] loss: 0.048 99.14872308462694 63.768844221105525\n",
      "epoch [19] loss: 0.023 99.79969954932399 63.71859296482412\n",
      "epoch [20] loss: 0.007 99.949924887331 64.42211055276383\n",
      "epoch [21] loss: 0.002 100.0 64.37185929648241\n",
      "epoch [22] loss: 0.001 100.0 64.67336683417085\n",
      "epoch [23] loss: 0.000 100.0 64.57286432160804\n",
      "epoch [24] loss: 0.000 100.0 64.42211055276383\n",
      "epoch [25] loss: 0.000 100.0 64.321608040201\n",
      "epoch [26] loss: 0.000 100.0 64.37185929648241\n",
      "epoch [27] loss: 0.000 100.0 64.2713567839196\n",
      "epoch [28] loss: 0.000 100.0 64.52261306532664\n",
      "epoch [29] loss: 0.000 100.0 64.42211055276383\n",
      "epoch [30] loss: 0.000 100.0 64.47236180904522\n",
      "Finished Training\n",
      "1586389550\n",
      "epoch [1] loss: 0.686 58.58895705521472 55.32663316582914\n",
      "epoch [2] loss: 0.658 61.29856850715746 60.80402010050251\n",
      "epoch [3] loss: 0.634 64.0081799591002 61.80904522613066\n",
      "epoch [4] loss: 0.602 73.26175869120654 65.07537688442211\n",
      "epoch [5] loss: 0.568 75.40899795501022 66.03015075376885\n",
      "epoch [6] loss: 0.513 76.32924335378323 66.78391959798995\n",
      "epoch [7] loss: 0.489 80.77709611451942 66.93467336683418\n",
      "epoch [8] loss: 0.432 84.91820040899796 67.1859296482412\n",
      "epoch [9] loss: 0.388 87.6278118609407 67.53768844221105\n",
      "epoch [10] loss: 0.323 90.03067484662577 67.93969849246231\n",
      "epoch [11] loss: 0.252 92.22903885480572 68.29145728643216\n",
      "epoch [12] loss: 0.212 92.7402862985685 66.83417085427136\n",
      "epoch [13] loss: 0.162 93.4560327198364 66.58291457286433\n",
      "epoch [14] loss: 0.120 98.26175869120654 68.29145728643216\n",
      "epoch [15] loss: 0.090 97.75051124744377 69.74874371859296\n",
      "epoch [16] loss: 0.074 98.92638036809817 67.53768844221105\n",
      "epoch [17] loss: 0.081 99.13087934560328 67.38693467336684\n",
      "epoch [18] loss: 0.039 99.59100204498978 68.34170854271356\n",
      "epoch [19] loss: 0.038 97.6482617586912 66.93467336683418\n",
      "epoch [20] loss: 0.095 98.41513292433538 67.63819095477388\n",
      "epoch [21] loss: 0.055 99.79550102249489 68.44221105527639\n",
      "epoch [22] loss: 0.010 100.0 67.8391959798995\n",
      "epoch [23] loss: 0.002 100.0 68.5929648241206\n",
      "epoch [24] loss: 0.001 100.0 68.44221105527639\n",
      "epoch [25] loss: 0.000 100.0 68.74371859296483\n",
      "epoch [26] loss: 0.000 100.0 68.09045226130654\n",
      "epoch [27] loss: 0.000 100.0 68.39195979899498\n",
      "epoch [28] loss: 0.000 100.0 68.34170854271356\n",
      "epoch [29] loss: 0.000 100.0 68.64321608040201\n",
      "epoch [30] loss: 0.000 100.0 68.29145728643216\n",
      "Finished Training\n",
      "epoch [1] loss: 0.690 60.69103655483225 58.14070351758794\n",
      "epoch [2] loss: 0.663 64.2964446670005 62.76381909547739\n",
      "epoch [3] loss: 0.630 69.75463194792188 63.969849246231156\n",
      "epoch [4] loss: 0.595 71.80771156735102 63.5678391959799\n",
      "epoch [5] loss: 0.565 76.81522283425137 65.07537688442211\n",
      "epoch [6] loss: 0.525 78.71807711567351 67.38693467336684\n",
      "epoch [7] loss: 0.484 82.97446169253881 67.23618090452261\n",
      "epoch [8] loss: 0.423 85.42814221331999 65.57788944723617\n",
      "epoch [9] loss: 0.361 87.63144717075613 67.38693467336684\n",
      "epoch [10] loss: 0.308 90.63595393089635 67.03517587939699\n",
      "epoch [11] loss: 0.236 94.04106159238859 65.92964824120602\n",
      "epoch [12] loss: 0.169 96.24436654982473 67.53768844221105\n",
      "epoch [13] loss: 0.124 95.89384076114172 66.73366834170854\n",
      "epoch [14] loss: 0.118 97.94692038057086 67.63819095477388\n",
      "epoch [15] loss: 0.069 98.84827240861291 66.28140703517587\n",
      "epoch [16] loss: 0.065 96.99549323985978 66.63316582914572\n",
      "epoch [17] loss: 0.077 99.19879819729594 65.22613065326634\n",
      "epoch [18] loss: 0.024 99.09864797195793 64.77386934673366\n",
      "epoch [19] loss: 0.051 99.79969954932399 66.18090452261306\n",
      "epoch [20] loss: 0.009 99.849774661993 65.62814070351759\n",
      "epoch [21] loss: 0.004 100.0 65.77889447236181\n",
      "epoch [22] loss: 0.001 100.0 66.33165829145729\n",
      "epoch [23] loss: 0.001 100.0 66.88442211055276\n",
      "epoch [24] loss: 0.000 100.0 66.88442211055276\n",
      "epoch [25] loss: 0.000 100.0 66.78391959798995\n",
      "epoch [26] loss: 0.000 100.0 66.68341708542714\n",
      "epoch [27] loss: 0.000 100.0 66.68341708542714\n",
      "epoch [28] loss: 0.000 100.0 66.78391959798995\n",
      "epoch [29] loss: 0.000 100.0 66.83417085427136\n",
      "epoch [30] loss: 0.000 100.0 66.98492462311557\n",
      "Finished Training\n",
      "1586389731\n",
      "epoch [1] loss: 0.695 52.86298568507157 54.221105527638194\n",
      "epoch [2] loss: 0.681 65.38854805725971 62.8643216080402\n",
      "epoch [3] loss: 0.637 65.84867075664621 62.914572864321606\n",
      "epoch [4] loss: 0.615 71.11451942740287 63.869346733668344\n",
      "epoch [5] loss: 0.581 72.6482617586912 65.62814070351759\n",
      "epoch [6] loss: 0.556 73.97750511247443 64.92462311557789\n",
      "epoch [7] loss: 0.509 77.19836400817996 65.678391959799\n",
      "epoch [8] loss: 0.466 83.23108384458078 65.47738693467336\n",
      "epoch [9] loss: 0.416 82.20858895705521 64.82412060301507\n",
      "epoch [10] loss: 0.392 87.21881390593047 66.38190954773869\n",
      "epoch [11] loss: 0.311 92.89366053169734 64.2713567839196\n",
      "epoch [12] loss: 0.226 92.79141104294479 64.87437185929649\n",
      "epoch [13] loss: 0.170 96.7280163599182 65.42713567839196\n",
      "epoch [14] loss: 0.105 96.67689161554192 64.321608040201\n",
      "epoch [15] loss: 0.101 98.41513292433538 65.87939698492463\n",
      "epoch [16] loss: 0.066 99.38650306748467 65.82914572864321\n",
      "epoch [17] loss: 0.050 99.13087934560328 64.82412060301507\n",
      "epoch [18] loss: 0.022 98.77300613496932 66.73366834170854\n",
      "epoch [19] loss: 0.048 98.97750511247443 65.92964824120602\n",
      "epoch [20] loss: 0.056 99.02862985685071 64.67336683417085\n",
      "epoch [21] loss: 0.058 99.43762781186093 64.57286432160804\n",
      "epoch [22] loss: 0.012 99.84662576687117 64.321608040201\n",
      "epoch [23] loss: 0.003 100.0 65.62814070351759\n",
      "epoch [24] loss: 0.001 100.0 66.23115577889448\n",
      "epoch [25] loss: 0.001 100.0 65.92964824120602\n",
      "epoch [26] loss: 0.000 100.0 65.82914572864321\n",
      "epoch [27] loss: 0.000 100.0 65.82914572864321\n",
      "epoch [28] loss: 0.000 100.0 65.77889447236181\n",
      "epoch [29] loss: 0.000 100.0 65.77889447236181\n",
      "epoch [30] loss: 0.000 100.0 65.62814070351759\n",
      "Finished Training\n",
      "epoch [1] loss: 0.686 57.83675513269905 55.62814070351759\n",
      "epoch [2] loss: 0.660 65.6484727090636 62.26130653266332\n",
      "epoch [3] loss: 0.628 70.10515773660491 61.80904522613066\n",
      "epoch [4] loss: 0.597 71.90786179268903 63.66834170854271\n",
      "epoch [5] loss: 0.568 75.11266900350526 65.97989949748744\n",
      "epoch [6] loss: 0.523 78.21732598898348 65.47738693467336\n",
      "epoch [7] loss: 0.467 73.86079118678018 63.618090452261306\n",
      "epoch [8] loss: 0.415 83.87581372058087 65.12562814070351\n",
      "epoch [9] loss: 0.355 90.43565348022032 66.68341708542714\n",
      "epoch [10] loss: 0.280 92.63895843765648 66.33165829145729\n",
      "epoch [11] loss: 0.220 91.3870806209314 63.91959798994975\n",
      "epoch [12] loss: 0.151 97.34601902854281 65.87939698492463\n",
      "epoch [13] loss: 0.093 98.29744616925387 65.17587939698493\n",
      "epoch [14] loss: 0.073 97.74661992989485 64.77386934673366\n",
      "epoch [15] loss: 0.108 97.19579369053581 65.07537688442211\n",
      "epoch [16] loss: 0.056 99.54932398597897 64.52261306532664\n",
      "epoch [17] loss: 0.016 99.79969954932399 65.92964824120602\n",
      "epoch [18] loss: 0.047 98.29744616925387 65.37688442211055\n",
      "epoch [19] loss: 0.079 98.84827240861291 64.62311557788945\n",
      "epoch [20] loss: 0.046 99.29894842263396 64.07035175879398\n",
      "epoch [21] loss: 0.021 99.89984977466199 65.77889447236181\n",
      "epoch [22] loss: 0.004 100.0 65.97989949748744\n",
      "epoch [23] loss: 0.001 100.0 65.82914572864321\n",
      "epoch [24] loss: 0.001 100.0 65.97989949748744\n",
      "epoch [25] loss: 0.001 100.0 65.92964824120602\n",
      "epoch [26] loss: 0.000 100.0 66.08040201005025\n",
      "epoch [27] loss: 0.000 100.0 66.18090452261306\n",
      "epoch [28] loss: 0.000 100.0 66.23115577889448\n",
      "epoch [29] loss: 0.000 100.0 66.28140703517587\n",
      "epoch [30] loss: 0.000 100.0 66.08040201005025\n",
      "Finished Training\n",
      "1586389911\n",
      "epoch [1] loss: 0.688 58.94683026584867 54.97487437185929\n",
      "epoch [2] loss: 0.662 64.21267893660531 60.15075376884422\n",
      "epoch [3] loss: 0.644 68.4560327198364 62.61306532663317\n",
      "epoch [4] loss: 0.626 69.3762781186094 62.06030150753769\n",
      "epoch [5] loss: 0.599 69.1717791411043 64.321608040201\n",
      "epoch [6] loss: 0.572 74.84662576687117 64.72361809045226\n",
      "epoch [7] loss: 0.537 74.079754601227 61.50753768844221\n",
      "epoch [8] loss: 0.510 79.03885480572598 65.678391959799\n",
      "epoch [9] loss: 0.465 84.86707566462168 67.23618090452261\n",
      "epoch [10] loss: 0.409 84.20245398773007 66.18090452261306\n",
      "epoch [11] loss: 0.344 90.23517382413088 64.72361809045226\n",
      "epoch [12] loss: 0.268 94.32515337423312 64.12060301507537\n",
      "epoch [13] loss: 0.196 96.2678936605317 66.38190954773869\n",
      "epoch [14] loss: 0.154 95.60327198364008 65.678391959799\n",
      "epoch [15] loss: 0.126 97.6482617586912 66.23115577889448\n",
      "epoch [16] loss: 0.061 99.18200408997956 65.07537688442211\n",
      "epoch [17] loss: 0.039 98.15950920245399 64.37185929648241\n",
      "epoch [18] loss: 0.101 97.6482617586912 64.72361809045226\n",
      "epoch [19] loss: 0.050 99.02862985685071 64.62311557788945\n",
      "epoch [20] loss: 0.044 98.8241308793456 64.07035175879398\n",
      "epoch [21] loss: 0.060 98.97750511247443 64.02010050251256\n",
      "epoch [22] loss: 0.039 99.48875255623722 63.969849246231156\n",
      "epoch [23] loss: 0.011 100.0 64.37185929648241\n",
      "epoch [24] loss: 0.002 100.0 64.72361809045226\n",
      "epoch [25] loss: 0.001 100.0 65.0251256281407\n",
      "epoch [26] loss: 0.001 100.0 65.0251256281407\n",
      "epoch [27] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [28] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [29] loss: 0.000 100.0 65.12562814070351\n",
      "epoch [30] loss: 0.000 100.0 65.07537688442211\n",
      "Finished Training\n",
      "epoch [1] loss: 0.686 61.64246369554331 59.64824120603015\n",
      "epoch [2] loss: 0.658 67.90185277916875 63.517587939698494\n",
      "epoch [3] loss: 0.627 68.15222834251377 63.91959798994975\n",
      "epoch [4] loss: 0.606 72.25838758137206 64.2713567839196\n",
      "epoch [5] loss: 0.564 75.41311967951928 63.06532663316583\n",
      "epoch [6] loss: 0.525 78.96845267901853 65.27638190954774\n",
      "epoch [7] loss: 0.473 82.37356034051076 64.52261306532664\n",
      "epoch [8] loss: 0.429 84.87731597396095 64.02010050251256\n",
      "epoch [9] loss: 0.364 89.18377566349524 66.38190954773869\n",
      "epoch [10] loss: 0.304 87.1807711567351 64.82412060301507\n",
      "epoch [11] loss: 0.244 91.83775663495243 63.71859296482412\n",
      "epoch [12] loss: 0.163 95.39308963445168 64.92462311557789\n",
      "epoch [13] loss: 0.180 97.64646970455684 65.27638190954774\n",
      "epoch [14] loss: 0.121 97.0956434651978 64.2713567839196\n",
      "epoch [15] loss: 0.070 97.74661992989485 63.91959798994975\n",
      "epoch [16] loss: 0.035 99.89984977466199 65.32663316582915\n",
      "epoch [17] loss: 0.009 99.89984977466199 64.37185929648241\n",
      "epoch [18] loss: 0.004 100.0 65.17587939698493\n",
      "epoch [19] loss: 0.001 100.0 65.0251256281407\n",
      "epoch [20] loss: 0.001 100.0 65.17587939698493\n",
      "epoch [21] loss: 0.000 100.0 65.0251256281407\n",
      "epoch [22] loss: 0.000 100.0 65.07537688442211\n",
      "epoch [23] loss: 0.000 100.0 64.9748743718593\n",
      "epoch [24] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [25] loss: 0.000 100.0 64.82412060301507\n",
      "epoch [26] loss: 0.000 100.0 64.77386934673366\n",
      "epoch [27] loss: 0.000 100.0 64.72361809045226\n",
      "epoch [28] loss: 0.000 100.0 64.87437185929649\n",
      "epoch [29] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [30] loss: 0.000 100.0 64.9748743718593\n",
      "Finished Training\n",
      "1586390092\n",
      "epoch [1] loss: 0.689 63.0879345603272 58.14070351758794\n",
      "epoch [2] loss: 0.661 68.14928425357873 62.36180904522613\n",
      "epoch [3] loss: 0.620 72.9038854805726 64.37185929648241\n",
      "epoch [4] loss: 0.574 70.91002044989776 65.57788944723617\n",
      "epoch [5] loss: 0.546 78.42535787321063 67.53768844221105\n",
      "epoch [6] loss: 0.502 80.26584867075664 65.82914572864321\n",
      "epoch [7] loss: 0.442 84.15132924335379 66.48241206030151\n",
      "epoch [8] loss: 0.387 88.8036809815951 66.58291457286433\n",
      "epoch [9] loss: 0.321 88.5480572597137 65.7286432160804\n",
      "epoch [10] loss: 0.250 92.68916155419222 65.27638190954774\n",
      "epoch [11] loss: 0.196 94.88752556237219 64.9748743718593\n",
      "epoch [12] loss: 0.171 95.44989775051124 65.47738693467336\n",
      "epoch [13] loss: 0.114 98.15950920245399 65.22613065326634\n",
      "epoch [14] loss: 0.076 96.62576687116564 67.73869346733669\n",
      "epoch [15] loss: 0.081 97.9038854805726 66.73366834170854\n",
      "epoch [16] loss: 0.080 98.67075664621677 65.22613065326634\n",
      "epoch [17] loss: 0.033 99.5398773006135 64.92462311557789\n",
      "epoch [18] loss: 0.030 99.13087934560328 65.678391959799\n",
      "epoch [19] loss: 0.074 99.33537832310839 66.53266331658291\n",
      "epoch [20] loss: 0.055 98.61963190184049 64.9748743718593\n",
      "epoch [21] loss: 0.028 99.94887525562372 65.82914572864321\n",
      "epoch [22] loss: 0.024 99.2842535787321 65.57788944723617\n",
      "epoch [23] loss: 0.027 99.69325153374233 64.67336683417085\n",
      "epoch [24] loss: 0.013 100.0 66.13065326633166\n",
      "epoch [25] loss: 0.002 100.0 65.7286432160804\n",
      "epoch [26] loss: 0.001 100.0 66.18090452261306\n",
      "epoch [27] loss: 0.000 100.0 66.13065326633166\n",
      "epoch [28] loss: 0.000 100.0 66.13065326633166\n",
      "epoch [29] loss: 0.000 100.0 66.28140703517587\n",
      "epoch [30] loss: 0.000 100.0 66.33165829145729\n",
      "Finished Training\n",
      "epoch [1] loss: 0.690 58.43765648472709 55.62814070351759\n",
      "epoch [2] loss: 0.670 66.04907361041562 59.54773869346734\n",
      "epoch [3] loss: 0.633 67.40110165247872 64.37185929648241\n",
      "epoch [4] loss: 0.608 70.45568352528794 65.22613065326634\n",
      "epoch [5] loss: 0.583 71.90786179268903 64.22110552763819\n",
      "epoch [6] loss: 0.547 76.21432148222334 64.02010050251256\n",
      "epoch [7] loss: 0.496 81.5723585378067 67.08542713567839\n",
      "epoch [8] loss: 0.438 81.72258387581373 65.37688442211055\n",
      "epoch [9] loss: 0.369 88.6830245368052 67.8894472361809\n",
      "epoch [10] loss: 0.301 92.58888332498748 65.17587939698493\n",
      "epoch [11] loss: 0.254 93.34001001502253 65.17587939698493\n",
      "epoch [12] loss: 0.185 91.03655483224837 64.17085427135679\n",
      "epoch [13] loss: 0.149 93.59038557836755 66.18090452261306\n",
      "epoch [14] loss: 0.134 96.44466700050076 64.2713567839196\n",
      "epoch [15] loss: 0.082 97.84677015523285 64.22110552763819\n",
      "epoch [16] loss: 0.074 98.14722083124687 65.0251256281407\n",
      "epoch [17] loss: 0.088 98.84827240861291 65.42713567839196\n",
      "epoch [18] loss: 0.026 100.0 66.48241206030151\n",
      "epoch [19] loss: 0.007 100.0 66.88442211055276\n",
      "epoch [20] loss: 0.002 100.0 66.4321608040201\n",
      "epoch [21] loss: 0.001 100.0 66.28140703517587\n",
      "epoch [22] loss: 0.001 100.0 66.13065326633166\n",
      "epoch [23] loss: 0.000 100.0 66.13065326633166\n",
      "epoch [24] loss: 0.000 100.0 66.33165829145729\n",
      "epoch [25] loss: 0.000 100.0 66.33165829145729\n",
      "epoch [26] loss: 0.000 100.0 66.33165829145729\n",
      "epoch [27] loss: 0.000 100.0 66.4321608040201\n",
      "epoch [28] loss: 0.000 100.0 66.53266331658291\n",
      "epoch [29] loss: 0.000 100.0 66.4321608040201\n",
      "epoch [30] loss: 0.000 100.0 66.48241206030151\n",
      "Finished Training\n",
      "1586390271\n",
      "epoch [1] loss: 0.688 60.787321063394685 55.778894472361806\n",
      "epoch [2] loss: 0.663 65.28629856850716 62.01005025125628\n",
      "epoch [3] loss: 0.639 64.57055214723927 62.96482412060301\n",
      "epoch [4] loss: 0.608 69.58077709611452 62.311557788944725\n",
      "epoch [5] loss: 0.581 74.43762781186093 65.07537688442211\n",
      "epoch [6] loss: 0.546 73.51738241308793 63.618090452261306\n",
      "epoch [7] loss: 0.517 80.93047034764827 64.9748743718593\n",
      "epoch [8] loss: 0.479 81.03271983640082 65.0251256281407\n",
      "epoch [9] loss: 0.419 85.22494887525562 65.678391959799\n",
      "epoch [10] loss: 0.352 89.97955010224949 65.678391959799\n",
      "epoch [11] loss: 0.312 91.46216768916156 64.9748743718593\n",
      "epoch [12] loss: 0.263 93.55828220858896 65.52763819095478\n",
      "epoch [13] loss: 0.201 92.48466257668711 62.36180904522613\n",
      "epoch [14] loss: 0.172 96.7280163599182 63.768844221105525\n",
      "epoch [15] loss: 0.118 97.80163599182004 63.66834170854271\n",
      "epoch [16] loss: 0.095 98.00613496932516 63.06532663316583\n",
      "epoch [17] loss: 0.060 97.85276073619632 64.02010050251256\n",
      "epoch [18] loss: 0.050 98.72188139059304 64.72361809045226\n",
      "epoch [19] loss: 0.064 98.15950920245399 65.62814070351759\n",
      "epoch [20] loss: 0.056 99.74437627811861 64.22110552763819\n",
      "epoch [21] loss: 0.046 99.48875255623722 64.57286432160804\n",
      "epoch [22] loss: 0.014 99.84662576687117 64.22110552763819\n",
      "epoch [23] loss: 0.012 98.61963190184049 63.91959798994975\n",
      "epoch [24] loss: 0.009 99.94887525562372 64.12060301507537\n",
      "epoch [25] loss: 0.003 100.0 65.0251256281407\n",
      "epoch [26] loss: 0.001 100.0 64.42211055276383\n",
      "epoch [27] loss: 0.000 100.0 64.47236180904522\n",
      "epoch [28] loss: 0.000 100.0 64.52261306532664\n",
      "epoch [29] loss: 0.000 100.0 64.42211055276383\n",
      "epoch [30] loss: 0.000 100.0 64.47236180904522\n",
      "Finished Training\n",
      "epoch [1] loss: 0.690 58.98848272408613 57.73869346733668\n",
      "epoch [2] loss: 0.675 64.2964446670005 62.51256281407035\n",
      "epoch [3] loss: 0.641 68.85327991987982 64.2713567839196\n",
      "epoch [4] loss: 0.613 72.05808713069604 65.678391959799\n",
      "epoch [5] loss: 0.575 75.21281922884326 66.98492462311557\n",
      "epoch [6] loss: 0.550 74.56184276414622 66.88442211055276\n",
      "epoch [7] loss: 0.510 76.56484727090636 67.73869346733669\n",
      "epoch [8] loss: 0.456 85.07761642463696 69.44723618090453\n",
      "epoch [9] loss: 0.414 84.87731597396095 67.03517587939699\n",
      "epoch [10] loss: 0.338 91.18678017025539 69.39698492462311\n",
      "epoch [11] loss: 0.270 93.64046069103655 66.53266331658291\n",
      "epoch [12] loss: 0.216 95.29293940911367 65.37688442211055\n",
      "epoch [13] loss: 0.149 96.09414121181773 65.37688442211055\n",
      "epoch [14] loss: 0.106 97.84677015523285 65.82914572864321\n",
      "epoch [15] loss: 0.115 96.49474211316975 63.5678391959799\n",
      "epoch [16] loss: 0.071 98.5978968452679 64.72361809045226\n",
      "epoch [17] loss: 0.039 99.69954932398598 66.88442211055276\n",
      "epoch [18] loss: 0.020 97.84677015523285 64.22110552763819\n",
      "epoch [19] loss: 0.115 96.44466700050076 65.678391959799\n",
      "epoch [20] loss: 0.064 99.64947421131697 64.87437185929649\n",
      "epoch [21] loss: 0.022 100.0 66.63316582914572\n",
      "epoch [22] loss: 0.006 100.0 66.68341708542714\n",
      "epoch [23] loss: 0.001 100.0 66.53266331658291\n",
      "epoch [24] loss: 0.001 100.0 66.48241206030151\n",
      "epoch [25] loss: 0.000 100.0 66.68341708542714\n",
      "epoch [26] loss: 0.000 100.0 66.68341708542714\n",
      "epoch [27] loss: 0.000 100.0 66.83417085427136\n",
      "epoch [28] loss: 0.000 100.0 66.78391959798995\n",
      "epoch [29] loss: 0.000 100.0 66.83417085427136\n",
      "epoch [30] loss: 0.000 100.0 66.78391959798995\n",
      "Finished Training\n",
      "1586390449\n",
      "epoch [1] loss: 0.690 59.25357873210634 55.32663316582914\n",
      "epoch [2] loss: 0.669 65.95092024539878 60.10050251256281\n",
      "epoch [3] loss: 0.631 69.32515337423312 63.31658291457286\n",
      "epoch [4] loss: 0.608 69.6319018404908 63.5678391959799\n",
      "epoch [5] loss: 0.561 75.4601226993865 64.02010050251256\n",
      "epoch [6] loss: 0.524 77.45398773006134 67.68844221105527\n",
      "epoch [7] loss: 0.493 81.23721881390593 67.28643216080403\n",
      "epoch [8] loss: 0.427 80.31697341513292 67.43718592964824\n",
      "epoch [9] loss: 0.394 84.50920245398773 65.82914572864321\n",
      "epoch [10] loss: 0.339 90.08179959100204 67.48743718592965\n",
      "epoch [11] loss: 0.236 93.60940695296523 65.62814070351759\n",
      "epoch [12] loss: 0.187 95.14314928425358 68.29145728643216\n",
      "epoch [13] loss: 0.140 97.08588957055214 68.49246231155779\n",
      "epoch [14] loss: 0.092 97.54601226993866 66.88442211055276\n",
      "epoch [15] loss: 0.119 99.13087934560328 66.03015075376885\n",
      "epoch [16] loss: 0.032 99.48875255623722 67.08542713567839\n",
      "epoch [17] loss: 0.019 100.0 66.98492462311557\n",
      "epoch [18] loss: 0.003 100.0 67.1859296482412\n",
      "epoch [19] loss: 0.001 100.0 67.03517587939699\n",
      "epoch [20] loss: 0.001 100.0 67.23618090452261\n",
      "epoch [21] loss: 0.001 100.0 67.03517587939699\n",
      "epoch [22] loss: 0.000 100.0 66.93467336683418\n",
      "epoch [23] loss: 0.000 100.0 66.88442211055276\n",
      "epoch [24] loss: 0.000 100.0 66.88442211055276\n",
      "epoch [25] loss: 0.000 100.0 66.93467336683418\n",
      "epoch [26] loss: 0.000 100.0 66.93467336683418\n",
      "epoch [27] loss: 0.000 100.0 66.93467336683418\n",
      "epoch [28] loss: 0.000 100.0 66.73366834170854\n",
      "epoch [29] loss: 0.000 100.0 66.78391959798995\n",
      "epoch [30] loss: 0.000 100.0 66.78391959798995\n",
      "Finished Training\n",
      "epoch [1] loss: 0.685 61.742613920881325 56.88442211055276\n",
      "epoch [2] loss: 0.656 66.59989984977466 59.597989949748744\n",
      "epoch [3] loss: 0.629 69.45418127190786 62.41206030150754\n",
      "epoch [4] loss: 0.586 73.15973960941412 63.266331658291456\n",
      "epoch [5] loss: 0.546 76.31447170756134 64.9748743718593\n",
      "epoch [6] loss: 0.505 80.22033049574361 66.48241206030151\n",
      "epoch [7] loss: 0.469 78.76815222834252 64.57286432160804\n",
      "epoch [8] loss: 0.422 87.23084626940411 69.39698492462311\n",
      "epoch [9] loss: 0.358 89.78467701552329 67.8391959798995\n",
      "epoch [10] loss: 0.296 91.18678017025539 67.23618090452261\n",
      "epoch [11] loss: 0.238 93.08963445167751 67.1859296482412\n",
      "epoch [12] loss: 0.181 93.89083625438157 67.78894472361809\n",
      "epoch [13] loss: 0.160 96.34451677516275 67.33668341708542\n",
      "epoch [14] loss: 0.103 98.24737105658488 67.38693467336684\n",
      "epoch [15] loss: 0.074 97.79669504256384 67.63819095477388\n",
      "epoch [16] loss: 0.074 96.89534301452179 66.38190954773869\n",
      "epoch [17] loss: 0.060 97.34601902854281 66.58291457286433\n",
      "epoch [18] loss: 0.094 97.44616925388083 65.77889447236181\n",
      "epoch [19] loss: 0.084 99.34902353530295 67.33668341708542\n",
      "epoch [20] loss: 0.030 99.59939909864798 67.78894472361809\n",
      "epoch [21] loss: 0.022 99.89984977466199 67.48743718592965\n",
      "epoch [22] loss: 0.007 99.949924887331 67.33668341708542\n",
      "epoch [23] loss: 0.002 100.0 67.48743718592965\n",
      "epoch [24] loss: 0.001 100.0 67.48743718592965\n",
      "epoch [25] loss: 0.000 100.0 67.28643216080403\n",
      "epoch [26] loss: 0.000 100.0 67.58793969849246\n",
      "epoch [27] loss: 0.000 100.0 67.43718592964824\n",
      "epoch [28] loss: 0.000 100.0 66.98492462311557\n",
      "epoch [29] loss: 0.000 100.0 67.43718592964824\n",
      "epoch [30] loss: 0.000 100.0 67.08542713567839\n",
      "Finished Training\n",
      "1586390629\n",
      "epoch [1] loss: 0.689 49.897750511247445 51.15577889447236\n",
      "epoch [2] loss: 0.671 66.87116564417178 62.41206030150754\n",
      "epoch [3] loss: 0.642 64.51942740286299 60.80402010050251\n",
      "epoch [4] loss: 0.613 71.98364008179959 63.869346733668344\n",
      "epoch [5] loss: 0.579 74.79550102249489 64.321608040201\n",
      "epoch [6] loss: 0.541 77.76073619631902 66.58291457286433\n",
      "epoch [7] loss: 0.508 81.33946830265849 67.03517587939699\n",
      "epoch [8] loss: 0.474 80.16359918200409 64.47236180904522\n",
      "epoch [9] loss: 0.431 86.04294478527608 67.53768844221105\n",
      "epoch [10] loss: 0.377 85.58282208588957 65.52763819095478\n",
      "epoch [11] loss: 0.317 91.51329243353783 65.47738693467336\n",
      "epoch [12] loss: 0.254 93.81390593047034 67.08542713567839\n",
      "epoch [13] loss: 0.205 96.06339468302659 65.7286432160804\n",
      "epoch [14] loss: 0.134 97.29038854805727 67.48743718592965\n",
      "epoch [15] loss: 0.107 97.85276073619632 65.12562814070351\n",
      "epoch [16] loss: 0.113 97.6482617586912 65.7286432160804\n",
      "epoch [17] loss: 0.107 95.96114519427402 65.92964824120602\n",
      "epoch [18] loss: 0.080 98.56850715746421 65.42713567839196\n",
      "epoch [19] loss: 0.054 98.3640081799591 66.03015075376885\n",
      "epoch [20] loss: 0.047 99.02862985685071 66.58291457286433\n",
      "epoch [21] loss: 0.056 99.079754601227 66.63316582914572\n",
      "epoch [22] loss: 0.041 99.64212678936606 66.03015075376885\n",
      "epoch [23] loss: 0.013 100.0 65.42713567839196\n",
      "epoch [24] loss: 0.032 98.72188139059304 65.32663316582915\n",
      "epoch [25] loss: 0.073 99.33537832310839 67.1859296482412\n",
      "epoch [26] loss: 0.012 99.84662576687117 67.33668341708542\n",
      "epoch [27] loss: 0.003 100.0 68.09045226130654\n",
      "epoch [28] loss: 0.001 100.0 67.68844221105527\n",
      "epoch [29] loss: 0.001 100.0 67.53768844221105\n",
      "epoch [30] loss: 0.000 100.0 67.58793969849246\n",
      "Finished Training\n",
      "epoch [1] loss: 0.689 59.539308963445166 55.678391959798994\n",
      "epoch [2] loss: 0.666 60.34051076614922 58.84422110552764\n",
      "epoch [3] loss: 0.651 67.75162744116174 63.41708542713568\n",
      "epoch [4] loss: 0.611 70.30545818728092 63.21608040201005\n",
      "epoch [5] loss: 0.588 68.7030545818728 60.653266331658294\n",
      "epoch [6] loss: 0.564 76.86529794692038 67.73869346733669\n",
      "epoch [7] loss: 0.513 77.51627441161743 63.21608040201005\n",
      "epoch [8] loss: 0.478 81.62243365047571 66.63316582914572\n",
      "epoch [9] loss: 0.436 86.07911867801702 67.78894472361809\n",
      "epoch [10] loss: 0.384 87.83174762143214 66.68341708542714\n",
      "epoch [11] loss: 0.322 88.6329494241362 66.28140703517587\n",
      "epoch [12] loss: 0.272 94.49173760640961 67.08542713567839\n",
      "epoch [13] loss: 0.193 94.84226339509264 65.47738693467336\n",
      "epoch [14] loss: 0.160 95.7436154231347 66.18090452261306\n",
      "epoch [15] loss: 0.098 97.1457185778668 66.4321608040201\n",
      "epoch [16] loss: 0.097 98.14722083124687 65.22613065326634\n",
      "epoch [17] loss: 0.075 98.44767150726089 65.62814070351759\n",
      "epoch [18] loss: 0.060 98.69804707060591 65.07537688442211\n",
      "epoch [19] loss: 0.093 99.14872308462694 65.47738693467336\n",
      "epoch [20] loss: 0.035 99.849774661993 65.92964824120602\n",
      "epoch [21] loss: 0.009 100.0 66.18090452261306\n",
      "epoch [22] loss: 0.033 99.74962443665498 66.08040201005025\n",
      "epoch [23] loss: 0.014 98.44767150726089 65.87939698492463\n",
      "epoch [24] loss: 0.026 99.14872308462694 65.87939698492463\n",
      "epoch [25] loss: 0.077 98.69804707060591 64.07035175879398\n",
      "epoch [26] loss: 0.045 98.34752128192288 64.72361809045226\n",
      "epoch [27] loss: 0.045 99.849774661993 64.77386934673366\n",
      "epoch [28] loss: 0.031 99.44917376064096 66.23115577889448\n",
      "epoch [29] loss: 0.042 99.24887330996495 65.0251256281407\n",
      "epoch [30] loss: 0.010 99.949924887331 65.92964824120602\n",
      "Finished Training\n",
      "1586390811\n",
      "epoch [1] loss: 0.689 60.37832310838446 57.58793969849246\n",
      "epoch [2] loss: 0.662 64.97955010224949 60.50251256281407\n",
      "epoch [3] loss: 0.636 68.09815950920246 62.914572864321606\n",
      "epoch [4] loss: 0.612 71.06339468302659 65.07537688442211\n",
      "epoch [5] loss: 0.588 73.51738241308793 65.0251256281407\n",
      "epoch [6] loss: 0.544 77.24948875255623 65.678391959799\n",
      "epoch [7] loss: 0.518 75.81799591002044 63.165829145728644\n",
      "epoch [8] loss: 0.489 81.5439672801636 64.22110552763819\n",
      "epoch [9] loss: 0.438 85.7361963190184 65.47738693467336\n",
      "epoch [10] loss: 0.384 81.44171779141104 63.869346733668344\n",
      "epoch [11] loss: 0.327 89.41717791411043 64.02010050251256\n",
      "epoch [12] loss: 0.264 92.84253578732107 63.81909547738694\n",
      "epoch [13] loss: 0.213 93.96728016359918 63.21608040201005\n",
      "epoch [14] loss: 0.196 94.98977505112474 65.52763819095478\n",
      "epoch [15] loss: 0.134 98.3640081799591 63.41708542713568\n",
      "epoch [16] loss: 0.091 98.77300613496932 66.28140703517587\n",
      "epoch [17] loss: 0.042 99.69325153374233 63.11557788944724\n",
      "epoch [18] loss: 0.040 98.67075664621677 64.87437185929649\n",
      "epoch [19] loss: 0.064 99.13087934560328 62.36180904522613\n",
      "epoch [20] loss: 0.031 99.64212678936606 64.87437185929649\n",
      "epoch [21] loss: 0.069 98.41513292433538 64.02010050251256\n",
      "epoch [22] loss: 0.049 99.64212678936606 63.969849246231156\n",
      "epoch [23] loss: 0.013 99.74437627811861 65.07537688442211\n",
      "epoch [24] loss: 0.046 98.92638036809817 63.46733668341709\n",
      "epoch [25] loss: 0.031 99.79550102249489 65.42713567839196\n",
      "epoch [26] loss: 0.004 100.0 65.12562814070351\n",
      "epoch [27] loss: 0.001 100.0 65.17587939698493\n",
      "epoch [28] loss: 0.001 100.0 65.52763819095478\n",
      "epoch [29] loss: 0.000 100.0 65.47738693467336\n",
      "epoch [30] loss: 0.000 100.0 65.47738693467336\n",
      "Finished Training\n",
      "epoch [1] loss: 0.689 60.44066099148723 56.58291457286432\n",
      "epoch [2] loss: 0.667 64.99749624436654 59.949748743718594\n",
      "epoch [3] loss: 0.631 69.70455683525287 64.37185929648241\n",
      "epoch [4] loss: 0.595 71.85778668002003 63.81909547738694\n",
      "epoch [5] loss: 0.562 75.36304456685028 64.2713567839196\n",
      "epoch [6] loss: 0.529 79.51927891837757 65.17587939698493\n",
      "epoch [7] loss: 0.485 78.31747621432149 64.9748743718593\n",
      "epoch [8] loss: 0.428 84.57686529794692 66.98492462311557\n",
      "epoch [9] loss: 0.400 85.17776664997496 66.18090452261306\n",
      "epoch [10] loss: 0.353 87.23084626940411 64.47236180904522\n",
      "epoch [11] loss: 0.290 91.28693039559339 66.33165829145729\n",
      "epoch [12] loss: 0.239 92.58888332498748 65.92964824120602\n",
      "epoch [13] loss: 0.178 97.04556835252879 65.97989949748744\n",
      "epoch [14] loss: 0.152 97.19579369053581 66.03015075376885\n",
      "epoch [15] loss: 0.107 94.2914371557336 65.77889447236181\n",
      "epoch [16] loss: 0.098 97.89684526790185 66.23115577889448\n",
      "epoch [17] loss: 0.086 95.29293940911367 64.92462311557789\n",
      "epoch [18] loss: 0.060 99.39909864797195 63.81909547738694\n",
      "epoch [19] loss: 0.034 97.89684526790185 64.82412060301507\n",
      "epoch [20] loss: 0.105 97.94692038057086 63.869346733668344\n",
      "epoch [21] loss: 0.037 99.59939909864798 64.17085427135679\n",
      "epoch [22] loss: 0.013 99.89984977466199 65.7286432160804\n",
      "epoch [23] loss: 0.003 100.0 65.52763819095478\n",
      "epoch [24] loss: 0.001 100.0 65.62814070351759\n",
      "epoch [25] loss: 0.001 100.0 65.7286432160804\n",
      "epoch [26] loss: 0.001 100.0 65.62814070351759\n",
      "epoch [27] loss: 0.000 100.0 65.57788944723617\n",
      "epoch [28] loss: 0.000 100.0 65.678391959799\n",
      "epoch [29] loss: 0.000 100.0 65.678391959799\n",
      "epoch [30] loss: 0.000 100.0 65.82914572864321\n",
      "Finished Training\n",
      "1586390993\n",
      "epoch [1] loss: 0.690 59.66257668711656 56.130653266331656\n",
      "epoch [2] loss: 0.664 62.67893660531697 59.54773869346734\n",
      "epoch [3] loss: 0.638 66.1042944785276 62.8643216080402\n",
      "epoch [4] loss: 0.612 71.98364008179959 64.17085427135679\n",
      "epoch [5] loss: 0.578 72.1881390593047 64.42211055276383\n",
      "epoch [6] loss: 0.532 77.65848670756647 66.08040201005025\n",
      "epoch [7] loss: 0.507 79.95910020449898 66.93467336683418\n",
      "epoch [8] loss: 0.441 85.5316973415133 66.68341708542714\n",
      "epoch [9] loss: 0.376 87.47443762781187 66.4321608040201\n",
      "epoch [10] loss: 0.310 90.6441717791411 65.32663316582915\n",
      "epoch [11] loss: 0.235 93.04703476482618 66.13065326633166\n",
      "epoch [12] loss: 0.167 97.39263803680981 65.678391959799\n",
      "epoch [13] loss: 0.106 97.85276073619632 64.321608040201\n",
      "epoch [14] loss: 0.088 98.61963190184049 65.77889447236181\n",
      "epoch [15] loss: 0.069 98.67075664621677 64.67336683417085\n",
      "epoch [16] loss: 0.075 97.1881390593047 65.52763819095478\n",
      "epoch [17] loss: 0.083 97.6482617586912 62.562814070351756\n",
      "epoch [18] loss: 0.049 99.59100204498978 62.71356783919598\n",
      "epoch [19] loss: 0.013 100.0 63.46733668341709\n",
      "epoch [20] loss: 0.003 100.0 64.02010050251256\n",
      "epoch [21] loss: 0.001 100.0 64.2713567839196\n",
      "epoch [22] loss: 0.001 100.0 64.62311557788945\n",
      "epoch [23] loss: 0.001 100.0 64.52261306532664\n",
      "epoch [24] loss: 0.000 100.0 64.67336683417085\n",
      "epoch [25] loss: 0.000 100.0 64.92462311557789\n",
      "epoch [26] loss: 0.000 100.0 64.82412060301507\n",
      "epoch [27] loss: 0.000 100.0 64.67336683417085\n",
      "epoch [28] loss: 0.000 100.0 64.72361809045226\n",
      "epoch [29] loss: 0.000 100.0 64.62311557788945\n",
      "epoch [30] loss: 0.000 100.0 64.72361809045226\n",
      "Finished Training\n",
      "epoch [1] loss: 0.685 61.792689033550324 58.84422110552764\n",
      "epoch [2] loss: 0.656 66.59989984977466 63.06532663316583\n",
      "epoch [3] loss: 0.618 63.59539308963445 59.447236180904525\n",
      "epoch [4] loss: 0.587 70.95643465197797 61.55778894472362\n",
      "epoch [5] loss: 0.552 77.0155232849274 66.63316582914572\n",
      "epoch [6] loss: 0.522 80.0200300450676 67.48743718592965\n",
      "epoch [7] loss: 0.470 83.72558838257386 66.23115577889448\n",
      "epoch [8] loss: 0.397 83.77566349524287 65.62814070351759\n",
      "epoch [9] loss: 0.320 89.93490235353029 65.57788944723617\n",
      "epoch [10] loss: 0.244 91.63745618427642 64.22110552763819\n",
      "epoch [11] loss: 0.180 96.89534301452179 66.08040201005025\n",
      "epoch [12] loss: 0.112 98.09714571857786 66.18090452261306\n",
      "epoch [13] loss: 0.073 99.04857285928894 65.12562814070351\n",
      "epoch [14] loss: 0.050 99.49924887330997 65.12562814070351\n",
      "epoch [15] loss: 0.037 98.94842263395093 64.9748743718593\n",
      "epoch [16] loss: 0.077 98.74812218327492 64.42211055276383\n",
      "epoch [17] loss: 0.061 99.59939909864798 66.28140703517587\n",
      "epoch [18] loss: 0.013 99.79969954932399 65.77889447236181\n",
      "epoch [19] loss: 0.004 100.0 65.52763819095478\n",
      "epoch [20] loss: 0.001 100.0 65.92964824120602\n",
      "epoch [21] loss: 0.001 100.0 66.08040201005025\n",
      "epoch [22] loss: 0.000 100.0 66.08040201005025\n",
      "epoch [23] loss: 0.000 100.0 65.92964824120602\n",
      "epoch [24] loss: 0.000 100.0 65.87939698492463\n",
      "epoch [25] loss: 0.000 100.0 66.18090452261306\n",
      "epoch [26] loss: 0.000 100.0 66.28140703517587\n",
      "epoch [27] loss: 0.000 100.0 66.33165829145729\n",
      "epoch [28] loss: 0.000 100.0 66.23115577889448\n",
      "epoch [29] loss: 0.000 100.0 66.28140703517587\n",
      "epoch [30] loss: 0.000 100.0 66.23115577889448\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# run train model 20 times, assign each experiment with a different random seed and save the output to a file\n",
    "\n",
    "import time\n",
    "import os\n",
    "for experiment_num in range(20):\n",
    "    t = int(time.time())\n",
    "    \n",
    "    dir_name = str(t)\n",
    "    print(dir_name)\n",
    "    #make directory\n",
    "    os.system(\"mkdir result\")\n",
    "    os.system(\"mkdir ./result/{}\".format(dir_name))\n",
    "    \n",
    "    #set random seed\n",
    "    torch.manual_seed(t)\n",
    "    np.random.seed(t)\n",
    "    \n",
    "    #train model without singan-generated images\n",
    "    use_singan = False\n",
    "    cnn, trainloader, testloader = train_model(use_singan, dir_name)\n",
    "    \n",
    "    \n",
    "    #start training data and save the print out to a file\n",
    "    \n",
    "    if use_singan:\n",
    "        filename = './result/{}/with_singan.txt'.format(dir_name)\n",
    "    else:\n",
    "        filename = './result/{}/without_singan.txt'.format(dir_name)\n",
    "    train(cnn, trainloader, testloader, 30, filename = filename)\n",
    "    \n",
    "    \n",
    "    #set random seed\n",
    "    torch.manual_seed(t)\n",
    "    np.random.seed(t)\n",
    "    \n",
    "    #train model with singan-generated images\n",
    "    use_singan = True\n",
    "    cnn, trainloader, testloader  = train_model(use_singan, dir_name)\n",
    "    \n",
    "    #start training data and save the print out to a file\n",
    "    \n",
    "    if use_singan:\n",
    "        filename = './result/{}/with_singan.txt'.format(dir_name)\n",
    "    else:\n",
    "        filename = './result/{}/without_singan.txt'.format(dir_name)\n",
    "    train(cnn, trainloader, testloader, 30, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
